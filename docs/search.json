[
  {
    "objectID": "aboutme.html",
    "href": "aboutme.html",
    "title": "About me",
    "section": "",
    "text": "A little about me\nHi everyone! My name is Katie Wells and I am a first year MPH student in the Epi concentration. I went to UGA for undergrad and majored in genetics; I also completed 2 years of undergraduate research in pertussis under Dr. Eric Harvill. I would love to study infectious disease epi and do more research in this field. I used R in undergrad in one course, and I just completed EPID 7500. I feel like there is always more to learn and understand in R no matter your skill level. I want to use this course to get more comfortable in programming and coding as I feel it is an integral part to almost every public health career. Something fun about me: I’ve had three hand surgeries in the past year all over a can of tomatoes (buy a decent can opener, everyone).\n\n\nHere’s a picture of me!\n\n\n\nIf you see me on campus say hey!\n\n\n\n\nSomething I find cool\nArtificial Intelligence for infectious disease Big Data Analytics - ScienceDirect\nI mentioned earlier that I am interested in infectious disease research; here’s an article I found about how AI can be used to increase accuracy in diagnosis, suggest treatments, and improve public health outcomes."
  },
  {
    "objectID": "coding_exercise.html",
    "href": "coding_exercise.html",
    "title": "R Coding Exercise",
    "section": "",
    "text": "#load dslabs packages and others\n#look at help file for gapminder data\n#get an overview of data structure\n#get a summary of data\n#determine the type of object gapminder is class(gapminder)\n#assign African coutnries to their own dataset\n#get an overview of africadata\n#get a summary of africadata\n#assign African countries’ infant mortality and life expectancy data to #its own object\n#assign African countries’ life expectancy and population data to its #own object\n#get an overview of africa_il and africa_lp\n#get a summary of africa_il and africa_lp\n#plot life expectancy as a function of infant mortality\n#from the plot there appears to be a negative correlation #between the two variables\n#plot life expectancy as a function of population size\n#from the plot there appears to be a #positive correlation between the two variables #the “streaks” of data #seem to come from the number of years avaiable for each country\n#find na values in infant_mortality\n#there is missing infant_mortality data up #until 1981 and again in 2016, so we should choose a year like 2000 to #avoid any NAs\n#make a dataset only including data from 2000\n#get overview and summary of africadata_2000\n#plot life expectancy as a function of infant mortality with 2000 data\n#from the plot there seems to be a negative correlation #between the two variables\n#plot life expectancy as a function of population size with 2000 data\n#from the plot there #is no clear correlation for the two variables in this dataset\n#fit 2000 data to a linear model with life_expectancy as the outcome and #infant_mortality as the predictor\n#get a summary of fit1\n#the p-value of 2.83e-08 show that #there is a significant correlation between life expectancy and infant #mortality in the 2000 dataset\n#fit 2000 data to a linear model with life_expectancy as the outcome and #population as the predictor\n#get a summary of fit2\n#the p-vlaue of 0.616 shows there #is not a significant correlation between life expectancy and population #in the 2000 dataset"
  },
  {
    "objectID": "coding_exercise.html#fitting-gdp-against-infant-mortality-and-population-in-a-linear-model",
    "href": "coding_exercise.html#fitting-gdp-against-infant-mortality-and-population-in-a-linear-model",
    "title": "R Coding Exercise",
    "section": "Fitting GDP against Infant Mortality and Population in a Linear Model",
    "text": "Fitting GDP against Infant Mortality and Population in a Linear Model\n\n## Feeling a little curious so I made some graphs...\n\n### Wonder what effects GDP on IM looks like...\n\nggplot2::ggplot(data = africadata_2000, mapping = aes(x = log(gdp),\n                                                      y = log(infant_mortality))) +\n  geom_point() + labs(x = \"Log GDP\", y = \"Log Infant Mortality\", \n                      title = \"Effects of GDP on Infant Mortality in 2000\")\n\n\n\n### Cool, cool. Now what if I do this with population...\nggplot2::ggplot(data = africadata_2000, mapping = aes(x = log(gdp),\n                                                      y = log(population))) +\n  geom_point() + labs(x = \"Log GDP\", y = \"Log Population\", \n                      title = \"Effects of GDP on Population in 2000\")\n\n\n\n#### ^^ NICEEEEE!\n\n\n## Fitting GDP against Infant Mortality\nfit3 <- lm(gdp ~ infant_mortality, data = africadata_2000)\n\n### Fit3 results\ntidy(fit3)\n\n# A tibble: 2 × 5\n  term                 estimate   std.error statistic  p.value\n  <chr>                   <dbl>       <dbl>     <dbl>    <dbl>\n1 (Intercept)      32124071647. 9128139601.      3.52 0.000945\n2 infant_mortality  -260618189.  107937461.     -2.41 0.0195  \n\n## Fitting GDP against Population\nfit4 <- lm(gdp ~ population, data=africadata_2000)\n\n### Fit 4 results\ntidy(fit4)\n\n# A tibble: 2 × 5\n  term           estimate   std.error statistic   p.value\n  <chr>             <dbl>       <dbl>     <dbl>     <dbl>\n1 (Intercept) 1620865586. 3499336123.     0.463 0.645    \n2 population         634.        130.     4.88  0.0000119\n\n\nFit 3 model suggests that there may be a negative association between GDP and Infant Mortality. Since the p-value is less than our alpha of 0.05 (p < 0.02), we find these results to be significant. Fit 3 model suggest that there may be a positive association between GDP and population. Since the p-value is less than our alpha of 0.05 (p < 0.001), we can reject the null hypothesis (no association)."
  },
  {
    "objectID": "dataanalysis_exercise.html",
    "href": "dataanalysis_exercise.html",
    "title": "My Data Analysis Portfolio",
    "section": "",
    "text": "This data was obtained from data.cdc.gov and contains data from 2/24/2022 to 1/26/2023. This dataset contains the same values used to display information available on the COVID Data Tracker, and is updated weekly. The CDC combines three metrics (new COVID-19 admissions per 100,000 population in the past 7 days, the percent of staffed inpatient beds occupied by COVID-19 patients, and total new COVID-19 cases per 100,000 population in the past 7 days) to determine the COVID-19 community level and classify it as low, medium, or high. This community level can help people and communities make decisions based on their circumstances and individual needs. It has a total of 12 columns and 158,000 rows including all available county data.\nI decided to keep the “date_updated” variable as there will be multiple observations for each county.\nI decided I needed to reduce the number of observations to make this data a little easier to use; I decided to filter by Georgia to bring this number down as well as give me some relatable data.\nThis is still a lot of observations, so I decided to filter to approximately a six-month period (07-28-2022 - 01-26-2023)\nI think it would be interesting to use this data analyze the number of COVID cases per 100k in relation to bed utilization and hospital admissions, as well as the number of cases per 100k over time to observe trends in infection. I don;t know the best way to incorporate it, but a graph (boxplot maybe?) including the community leel would also be cool to see. These analyses could tell us something about COVID 19 case trends in Georgia during these last 6 months; as the pandemic draws on after almost 3 year it would be interesting to see what level of community severity still exists."
  },
  {
    "objectID": "dataanalysis_exercise.html#load-cleaned-data-and-load-necessary-libraries",
    "href": "dataanalysis_exercise.html#load-cleaned-data-and-load-necessary-libraries",
    "title": "My Data Analysis Portfolio",
    "section": "Load Cleaned Data and Load Necessary Libraries",
    "text": "Load Cleaned Data and Load Necessary Libraries\n\nclean_data <- readRDS(\"community.rds\")\nlibrary(tidyverse)"
  },
  {
    "objectID": "dataanalysis_exercise.html#data-visualization-covid-19-in-columbia-county",
    "href": "dataanalysis_exercise.html#data-visualization-covid-19-in-columbia-county",
    "title": "My Data Analysis Portfolio",
    "section": "Data Visualization: COVID-19 in Columbia County",
    "text": "Data Visualization: COVID-19 in Columbia County\n\n# Seeing COVID-19 Cases per 100K over Time\nggplot(clean_data %>% filter(county == \"Columbia County\"), aes(x = date_updated, y = covid_cases_per_100k)) + geom_line() + labs(x = \"Date\", y = \"Cases Per 100K\")\n\n\n\n# Boxplots of Inpatient Bed Utilization vs COVID Cases per 100K by Threat Level\nclean_data <- clean_data %>% rename(Threat_Level = `covid-19_community_level`) %>% mutate(Threat_Level = factor(Threat_Level, levels = c(\"Low\", \"Medium\", \"High\")))\n\nggplot(clean_data %>% filter(county == \"Columbia County\"), aes(x = covid_cases_per_100k, y = covid_inpatient_bed_utilization, group = Threat_Level, fill = Threat_Level)) + geom_boxplot() + labs(x = \"Covid Cases per 100K\", y = \"COVID Inpatient Bed Utilization\")"
  },
  {
    "objectID": "dataanalysis_exercise.html#data-visualization-covid-19-in-georgia-counties-that-start-wthe-letter-c",
    "href": "dataanalysis_exercise.html#data-visualization-covid-19-in-georgia-counties-that-start-wthe-letter-c",
    "title": "My Data Analysis Portfolio",
    "section": "Data Visualization: COVID-19 in Georgia Counties that Start w/the Letter C",
    "text": "Data Visualization: COVID-19 in Georgia Counties that Start w/the Letter C\n\n# What Counties Have the Most COVID-19 Hospital Admissions?\nggplot(clean_data %>% filter(substr(county, 1, 1) == \"C\"), aes(x = covid_hospital_admissions_per_100k, y = county)) + geom_col()"
  },
  {
    "objectID": "fluanalysis/code/exploration.html",
    "href": "fluanalysis/code/exploration.html",
    "title": "My Data Analysis Portfolio",
    "section": "",
    "text": "Exploration\nLoading required packages and cleaned data.\n\n#load required packages\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.0     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.1     ✔ tibble    3.1.8\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nlibrary(here)\n\nhere() starts at C:/Users/Katie/Documents/2022-2023/MADA/katiewells-MADA-portfolio\n\n\n\n#load data\nflu2 <- readRDS(here(\"fluanalysis\", \"data\", \"flu2.rds\"))\n\nSummary statistics for BodyTemp and Nausea\n\n#provide summary data for important variables\nflu2 %>% pull(Nausea) %>% summary()\n\n No Yes \n475 255 \n\nflu2 %>% pull(BodyTemp) %>% summary()\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  97.20   98.20   98.50   98.94   99.30  103.10 \n\n\nDistribution of BodyTemp\n\n#look at the distribution of BodyTemp\nflu2 %>% ggplot(aes(x=BodyTemp)) + geom_histogram() \n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nLooks like of the body temperatures cluster around 98.5 degrees with some right skew.\nLets take a look ath the relationship between BodyTemp and some predictors.\n\nflu2 %>% ggplot(aes(x=Nausea, y=BodyTemp)) + geom_boxplot()\n\n\n\n\nSeems like median body temperature is just slightly higher in those with nausea than without.\n\nflu2 %>% ggplot(aes(x=CoughIntensity, y=BodyTemp)) + geom_boxplot()\n\n\n\n\nMedian body temperature looks to increase as cough intensity increases.\n\nflu2 %>% ggplot(aes(x=Sneeze, y=BodyTemp)) + geom_boxplot()\n\n\n\n\nLooks like people who did not report sneezing have a higher median body temperature. Weird.\n\nflu2 %>% ggplot(aes(x=Weakness, y=BodyTemp)) + geom_boxplot()\n\n\n\n\nMedian body temperature seems to increase as weakness increases."
  },
  {
    "objectID": "fluanalysis/code/fitting.html",
    "href": "fluanalysis/code/fitting.html",
    "title": "My Data Analysis Portfolio",
    "section": "",
    "text": "Model Fitting\n\nLoad packages and cleaned data\n\n\n#load required packages\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.0     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.1     ✔ tibble    3.1.8\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nlibrary(here)\n\nhere() starts at C:/Users/Katie/Documents/2022-2023/MADA/katiewells-MADA-portfolio\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.0.0 ──\n✔ broom        1.0.3     ✔ rsample      1.1.1\n✔ dials        1.1.0     ✔ tune         1.0.1\n✔ infer        1.0.4     ✔ workflows    1.1.3\n✔ modeldata    1.1.0     ✔ workflowsets 1.0.0\n✔ parsnip      1.0.4     ✔ yardstick    1.1.0\n✔ recipes      1.0.5     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Search for functions across packages at https://www.tidymodels.org/find/\n\nlibrary(performance)\n\n\nAttaching package: 'performance'\n\nThe following objects are masked from 'package:yardstick':\n\n    mae, rmse\n\n\n\n#load data\nflu2 <- readRDS(here(\"fluanalysis\", \"data\", \"flu2.rds\"))\n\n\nLinear Modeling with Main Predictor Here we will use tidymodels to fit a linear model to BodyTemp, first with just the main predictor RunnyNose and later with all predictors.\n\n\n#specify functional form of model\nlm_mod <- linear_reg() %>% set_engine(\"lm\") \n#Fit a linear model to the continuous outcome (BodyTemp) using only the main predictor of interest (RunnyNose)\nlm_fit <- lm_mod %>% fit(BodyTemp ~ RunnyNose, data=flu2)\nlm_fit\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = BodyTemp ~ RunnyNose, data = data)\n\nCoefficients:\n (Intercept)  RunnyNoseYes  \n     99.1431       -0.2926  \n\n\n\n#take a look at the results of lm_fit\nglance(lm_fit)\n\n# A tibble: 1 × 12\n  r.squ…¹ adj.r…² sigma stati…³ p.value    df logLik   AIC   BIC devia…⁴ df.re…⁵\n    <dbl>   <dbl> <dbl>   <dbl>   <dbl> <dbl>  <dbl> <dbl> <dbl>   <dbl>   <int>\n1  0.0123  0.0110  1.19    9.08 0.00268     1 -1162. 2329. 2343.   1031.     728\n# … with 1 more variable: nobs <int>, and abbreviated variable names\n#   ¹​r.squared, ²​adj.r.squared, ³​statistic, ⁴​deviance, ⁵​df.residual\n\n\n\nLinear Modeling with all predictors\n\n\n#Fit a linear model to the continuous outcome (BodyTemp) using all predictors\nlm_fit2 <- lm_mod %>% \n          fit(BodyTemp ~ ., data = flu2)\nlm_fit2\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = BodyTemp ~ ., data = data)\n\nCoefficients:\n         (Intercept)  SwollenLymphNodesYes    ChestCongestionYes  \n           98.268642             -0.166066              0.102608  \n     ChillsSweatsYes    NasalCongestionYes             SneezeYes  \n            0.191826             -0.221568             -0.372709  \n          FatigueYes    SubjectiveFeverYes           HeadacheYes  \n            0.272509              0.436694              0.004977  \n          Weakness.L            Weakness.Q            Weakness.C  \n            0.254046              0.128981              0.023351  \n    CoughIntensity.L      CoughIntensity.Q      CoughIntensity.C  \n            0.170659             -0.166954              0.129975  \n           Myalgia.L             Myalgia.Q             Myalgia.C  \n           -0.132614             -0.131992              0.093532  \n        RunnyNoseYes             AbPainYes          ChestPainYes  \n           -0.064666              0.015375              0.100308  \n         DiarrheaYes              EyePnYes           InsomniaYes  \n           -0.152344              0.128786             -0.005824  \n         ItchyEyeYes             NauseaYes              EarPnYes  \n           -0.003889             -0.033246              0.111125  \n      PharyngitisYes         BreathlessYes            ToothPnYes  \n            0.315991              0.086379             -0.035497  \n            VomitYes             WheezeYes  \n            0.160053             -0.034654  \n\n\n\n#take a look at the results of lm_fit2\nglance(lm_fit2)\n\n# A tibble: 1 × 12\n  r.squ…¹ adj.r…² sigma stati…³ p.value    df logLik   AIC   BIC devia…⁴ df.re…⁵\n    <dbl>   <dbl> <dbl>   <dbl>   <dbl> <dbl>  <dbl> <dbl> <dbl>   <dbl>   <int>\n1   0.124  0.0853  1.14    3.19 2.47e-8    31 -1118. 2302. 2453.    914.     698\n# … with 1 more variable: nobs <int>, and abbreviated variable names\n#   ¹​r.squared, ²​adj.r.squared, ³​statistic, ⁴​deviance, ⁵​df.residual\n\n\n\nComparing models I was struggling to figure out how to compare these models with code. I did some searching and came across the package performance(). I’m going to use the compare_performance() function to do this.\n\n\ncompare_performance(lm_fit, lm_fit2)\n\n# Comparison of Model Performance Indices\n\nName    | Model |  AIC (weights) | AICc (weights) |  BIC (weights) |    R2 | R2 (adj.) |  RMSE | Sigma\n------------------------------------------------------------------------------------------------------\nlm_fit  |   _lm | 2329.3 (<.001) | 2329.4 (<.001) | 2343.1 (>.999) | 0.012 |     0.011 | 1.188 | 1.190\nlm_fit2 |   _lm | 2301.6 (>.999) | 2304.8 (>.999) | 2453.1 (<.001) | 0.124 |     0.085 | 1.119 | 1.144\n\n\n\nLogistic Modeling with Main predictor Here we will use tidymodels to fit a logistic model to Nausea, first with just the main predictor RunnyNose and later with all predictors.\n\n\n#specify functional form of model\nglm_mod <- logistic_reg() %>%\n  set_engine(\"glm\")\n#Fit a logistic model to the categorical outcome (Nausea) using only the main predictor of interest (RunnyNose)\nglm_fit <- glm_mod %>% \n          fit(Nausea ~ RunnyNose, data = flu2)\nglm_fit\n\nparsnip model object\n\n\nCall:  stats::glm(formula = Nausea ~ RunnyNose, family = stats::binomial, \n    data = data)\n\nCoefficients:\n (Intercept)  RunnyNoseYes  \n    -0.65781       0.05018  \n\nDegrees of Freedom: 729 Total (i.e. Null);  728 Residual\nNull Deviance:      944.7 \nResidual Deviance: 944.6    AIC: 948.6\n\n\n\n#take a look athe the results of glm_fit\nglance(glm_fit)\n\n# A tibble: 1 × 8\n  null.deviance df.null logLik   AIC   BIC deviance df.residual  nobs\n          <dbl>   <int>  <dbl> <dbl> <dbl>    <dbl>       <int> <int>\n1          945.     729  -472.  949.  958.     945.         728   730\n\n\n\nLogistic modeling with all predictors\n\n\n#Fit a logistic model to the categorical outcome (Nausea) using all predictors\nglm_fit2 <- glm_mod %>% \n          fit(Nausea ~ ., data = flu2)\nglm_fit2\n\nparsnip model object\n\n\nCall:  stats::glm(formula = Nausea ~ ., family = stats::binomial, data = data)\n\nCoefficients:\n         (Intercept)  SwollenLymphNodesYes    ChestCongestionYes  \n             0.05966              -0.24685               0.26529  \n     ChillsSweatsYes    NasalCongestionYes             SneezeYes  \n             0.29057               0.44007               0.16502  \n          FatigueYes    SubjectiveFeverYes           HeadacheYes  \n             0.23096               0.25502               0.33749  \n          Weakness.L            Weakness.Q            Weakness.C  \n             0.65600               0.31281              -0.11280  \n    CoughIntensity.L      CoughIntensity.Q      CoughIntensity.C  \n            -0.77141              -0.11999              -0.13857  \n           Myalgia.L             Myalgia.Q             Myalgia.C  \n             0.13177              -0.03511              -0.10548  \n        RunnyNoseYes             AbPainYes          ChestPainYes  \n             0.05264               0.94153               0.07153  \n         DiarrheaYes              EyePnYes           InsomniaYes  \n             1.06479              -0.32772               0.08410  \n         ItchyEyeYes              EarPnYes        PharyngitisYes  \n            -0.05269              -0.16544               0.29154  \n       BreathlessYes            ToothPnYes              VomitYes  \n             0.53063               0.48015               2.44716  \n           WheezeYes              BodyTemp  \n            -0.27581              -0.03136  \n\nDegrees of Freedom: 729 Total (i.e. Null);  698 Residual\nNull Deviance:      944.7 \nResidual Deviance: 752.1    AIC: 816.1\n\n\n\nglance(glm_fit2)\n\n# A tibble: 1 × 8\n  null.deviance df.null logLik   AIC   BIC deviance df.residual  nobs\n          <dbl>   <int>  <dbl> <dbl> <dbl>    <dbl>       <int> <int>\n1          945.     729  -376.  816.  963.     752.         698   730\n\n\n\nComparing models\n\n\ncompare_performance(glm_fit, glm_fit2)\n\n# Comparison of Model Performance Indices\n\nName     | Model | AIC (weights) | AICc (weights) | BIC (weights) | Tjur's R2 |  RMSE | Sigma | Log_loss | Score_log | Score_spherical |   PCP\n----------------------------------------------------------------------------------------------------------------------------------------------\nglm_fit  |  _glm | 948.6 (<.001) |  948.6 (<.001) | 957.8 (0.936) | 1.169e-04 | 0.477 | 1.139 |    0.647 |  -107.871 |           0.012 | 0.545\nglm_fit2 |  _glm | 816.1 (>.999) |  819.2 (>.999) | 963.1 (0.064) |     0.246 | 0.415 | 1.038 |    0.515 |      -Inf |           0.002 | 0.657\n\n\nThe model with all predictors has a lower AIC, so it appears to be the better model."
  },
  {
    "objectID": "fluanalysis/code/machinelearning.html",
    "href": "fluanalysis/code/machinelearning.html",
    "title": "My Data Analysis Portfolio",
    "section": "",
    "text": "Machine Learning\nLoading the needed packages and the raw data.\nSetup\nMaking recipe\nNull model"
  },
  {
    "objectID": "fluanalysis/code/machinelearning.html#training-data",
    "href": "fluanalysis/code/machinelearning.html#training-data",
    "title": "My Data Analysis Portfolio",
    "section": "training data",
    "text": "training data\n\n#null model recipe with training data\nnull_recipe_train <- recipe(BodyTemp ~ 1, data = flu_train1)\n\nnull_wf_train <- workflow() %>% add_model(null_mod) %>% add_recipe(null_recipe_train)\n\nnull_train_fit <- \n  fit_resamples(null_wf_train, resamples = folds_train)"
  },
  {
    "objectID": "fluanalysis/code/machinelearning.html#testing-data",
    "href": "fluanalysis/code/machinelearning.html#testing-data",
    "title": "My Data Analysis Portfolio",
    "section": "testing data",
    "text": "testing data\n\n#null model recipe with testing data\nnull_recipe_test <- recipe(BodyTemp ~ 1, data = flu_test1)\n\nnull_wf_test <- workflow() %>% add_model(null_mod) %>% add_recipe(null_recipe_test)\n\nnull_test_fit <- \n  fit_resamples(null_wf_test, resamples = folds_test)\n\n\n#collect metrics from null \nnull_train_fit %>% collect_metrics()\n\n# A tibble: 2 × 6\n  .metric .estimator   mean     n std_err .config             \n  <chr>   <chr>       <dbl> <int>   <dbl> <chr>               \n1 rmse    standard     1.21    25  0.0177 Preprocessor1_Model1\n2 rsq     standard   NaN        0 NA      Preprocessor1_Model1\n\nnull_test_fit %>% collect_metrics()\n\n# A tibble: 2 × 6\n  .metric .estimator   mean     n std_err .config             \n  <chr>   <chr>       <dbl> <int>   <dbl> <chr>               \n1 rmse    standard     1.16    25  0.0285 Preprocessor1_Model1\n2 rsq     standard   NaN        0 NA      Preprocessor1_Model1\n\n\nModel Tuning and Fitting\nTree"
  },
  {
    "objectID": "fluanalysis/code/machinelearning.html#model-specification",
    "href": "fluanalysis/code/machinelearning.html#model-specification",
    "title": "My Data Analysis Portfolio",
    "section": "Model Specification",
    "text": "Model Specification\n\ntune_spec <- decision_tree(cost_complexity = tune(),\n                           tree_depth = tune()) %>%\n  set_engine(\"rpart\") %>%\n  set_mode(\"regression\")\ntune_spec\n\nDecision Tree Model Specification (regression)\n\nMain Arguments:\n  cost_complexity = tune()\n  tree_depth = tune()\n\nComputational engine: rpart"
  },
  {
    "objectID": "fluanalysis/code/machinelearning.html#workflow-definition",
    "href": "fluanalysis/code/machinelearning.html#workflow-definition",
    "title": "My Data Analysis Portfolio",
    "section": "Workflow Definition",
    "text": "Workflow Definition\n\ntree_wf <- workflow() %>%\n  add_model(tune_spec) %>%\n  add_recipe(flu_rec)"
  },
  {
    "objectID": "fluanalysis/code/machinelearning.html#tuning-grid-specification",
    "href": "fluanalysis/code/machinelearning.html#tuning-grid-specification",
    "title": "My Data Analysis Portfolio",
    "section": "Tuning Grid Specification",
    "text": "Tuning Grid Specification\n\ntree_grid <- grid_regular(cost_complexity(),\n                          tree_depth(),\n                          levels = 5)\ntree_grid\n\n# A tibble: 25 × 2\n   cost_complexity tree_depth\n             <dbl>      <int>\n 1    0.0000000001          1\n 2    0.0000000178          1\n 3    0.00000316            1\n 4    0.000562              1\n 5    0.1                   1\n 6    0.0000000001          4\n 7    0.0000000178          4\n 8    0.00000316            4\n 9    0.000562              4\n10    0.1                   4\n# … with 15 more rows\n\ntree_grid %>%\n  count(tree_depth)\n\n# A tibble: 5 × 2\n  tree_depth     n\n       <int> <int>\n1          1     5\n2          4     5\n3          8     5\n4         11     5\n5         15     5"
  },
  {
    "objectID": "fluanalysis/code/machinelearning.html#tuning-using-cross-validation-and-tune_grid",
    "href": "fluanalysis/code/machinelearning.html#tuning-using-cross-validation-and-tune_grid",
    "title": "My Data Analysis Portfolio",
    "section": "Tuning using Cross-Validation and Tune_grid()",
    "text": "Tuning using Cross-Validation and Tune_grid()\n\ntree_res <- tree_wf %>%\n  tune_grid(\n    resamples = folds_train,\n    grid = tree_grid\n  )\n\n! Fold1, Repeat1: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\n! Fold2, Repeat1: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\n! Fold3, Repeat1: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\n! Fold4, Repeat1: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\n! Fold5, Repeat1: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\n! Fold1, Repeat2: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\n! Fold2, Repeat2: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\n! Fold3, Repeat2: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\n! Fold4, Repeat2: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\n! Fold5, Repeat2: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\n! Fold1, Repeat3: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\n! Fold2, Repeat3: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\n! Fold3, Repeat3: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\n! Fold4, Repeat3: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\n! Fold5, Repeat3: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\n! Fold1, Repeat4: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\n! Fold2, Repeat4: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\n! Fold3, Repeat4: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\n! Fold4, Repeat4: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\n! Fold5, Repeat4: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\n! Fold1, Repeat5: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\n! Fold2, Repeat5: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\n! Fold3, Repeat5: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\n! Fold4, Repeat5: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\n! Fold5, Repeat5: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\ntree_res\n\n# Tuning results\n# 5-fold cross-validation repeated 5 times using stratification \n# A tibble: 25 × 5\n   splits            id      id2   .metrics          .notes          \n   <list>            <chr>   <chr> <list>            <list>          \n 1 <split [405/103]> Repeat1 Fold1 <tibble [50 × 6]> <tibble [1 × 3]>\n 2 <split [405/103]> Repeat1 Fold2 <tibble [50 × 6]> <tibble [1 × 3]>\n 3 <split [406/102]> Repeat1 Fold3 <tibble [50 × 6]> <tibble [1 × 3]>\n 4 <split [408/100]> Repeat1 Fold4 <tibble [50 × 6]> <tibble [1 × 3]>\n 5 <split [408/100]> Repeat1 Fold5 <tibble [50 × 6]> <tibble [1 × 3]>\n 6 <split [405/103]> Repeat2 Fold1 <tibble [50 × 6]> <tibble [1 × 3]>\n 7 <split [405/103]> Repeat2 Fold2 <tibble [50 × 6]> <tibble [1 × 3]>\n 8 <split [406/102]> Repeat2 Fold3 <tibble [50 × 6]> <tibble [1 × 3]>\n 9 <split [408/100]> Repeat2 Fold4 <tibble [50 × 6]> <tibble [1 × 3]>\n10 <split [408/100]> Repeat2 Fold5 <tibble [50 × 6]> <tibble [1 × 3]>\n# … with 15 more rows\n\nThere were issues with some computations:\n\n  - Warning(s) x25: There was 1 warning in `dplyr::summarise()`. ℹ In argument: `.est...\n\nRun `show_notes(.Last.tune.result)` for more information.\n\ntree_res %>%\n  collect_metrics()\n\n# A tibble: 50 × 8\n   cost_complexity tree_depth .metric .estimator     mean     n  std_err .config\n             <dbl>      <int> <chr>   <chr>         <dbl> <int>    <dbl> <chr>  \n 1    0.0000000001          1 rmse    standard     1.19      25  0.0181  Prepro…\n 2    0.0000000001          1 rsq     standard     0.0361    25  0.00422 Prepro…\n 3    0.0000000178          1 rmse    standard     1.19      25  0.0181  Prepro…\n 4    0.0000000178          1 rsq     standard     0.0361    25  0.00422 Prepro…\n 5    0.00000316            1 rmse    standard     1.19      25  0.0181  Prepro…\n 6    0.00000316            1 rsq     standard     0.0361    25  0.00422 Prepro…\n 7    0.000562              1 rmse    standard     1.19      25  0.0181  Prepro…\n 8    0.000562              1 rsq     standard     0.0361    25  0.00422 Prepro…\n 9    0.1                   1 rmse    standard     1.21      25  0.0177  Prepro…\n10    0.1                   1 rsq     standard   NaN          0 NA       Prepro…\n# … with 40 more rows\n\n\n\n#plot using autoplot\ntree_res %>% autoplot()\n\n\n\n\n\n#getting best model\ntree_res %>%\n  show_best(metric = \"rmse\")\n\n# A tibble: 5 × 8\n  cost_complexity tree_depth .metric .estimator  mean     n std_err .config     \n            <dbl>      <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>       \n1    0.0000000001          1 rmse    standard    1.19    25  0.0181 Preprocesso…\n2    0.0000000178          1 rmse    standard    1.19    25  0.0181 Preprocesso…\n3    0.00000316            1 rmse    standard    1.19    25  0.0181 Preprocesso…\n4    0.000562              1 rmse    standard    1.19    25  0.0181 Preprocesso…\n5    0.0000000001          4 rmse    standard    1.20    25  0.0182 Preprocesso…\n\nbest_tree <- tree_res %>%\n  select_best(metric = \"rmse\")\nbest_tree\n\n# A tibble: 1 × 3\n  cost_complexity tree_depth .config              \n            <dbl>      <int> <chr>                \n1    0.0000000001          1 Preprocessor1_Model01\n\n\nFinalizing fit with best model\n\n#finalizing workflows with best models\nfinal_tree_wf <- tree_wf %>%\n  finalize_workflow(best_tree)\n\nfinal_tree_fit <- final_tree_wf %>% fit(data=flu_train1)\nfinal_tree_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\nn= 508 \n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n1) root 508 742.9363 98.93642  \n  2) Sneeze_Yes>=0.5 280 259.6477 98.69107 *\n  3) Sneeze_Yes< 0.5 228 445.7356 99.23772 *\n\n\n\n#plot tree\nrpart.plot(extract_fit_parsnip(final_tree_fit)$fit)\n\nWarning: Cannot retrieve the data used to build the model (so cannot determine roundint and is.binary for the variables).\nTo silence this warning:\n    Call rpart.plot with roundint=FALSE,\n    or rebuild the rpart model with model=TRUE."
  },
  {
    "objectID": "fluanalysis/code/machinelearning.html#model-specification-1",
    "href": "fluanalysis/code/machinelearning.html#model-specification-1",
    "title": "My Data Analysis Portfolio",
    "section": "Model Specification",
    "text": "Model Specification\n\nlr_mod <- linear_reg(penalty = tune(), mixture = 1) %>%\n  set_engine(\"glmnet\")"
  },
  {
    "objectID": "fluanalysis/code/machinelearning.html#workflow-definition-1",
    "href": "fluanalysis/code/machinelearning.html#workflow-definition-1",
    "title": "My Data Analysis Portfolio",
    "section": "Workflow Definition",
    "text": "Workflow Definition\n\nlr_workflow <- workflow() %>% \n  add_model(lr_mod) %>% \n  add_recipe(flu_rec)"
  },
  {
    "objectID": "fluanalysis/code/machinelearning.html#tuning-grid-specification-1",
    "href": "fluanalysis/code/machinelearning.html#tuning-grid-specification-1",
    "title": "My Data Analysis Portfolio",
    "section": "Tuning Grid Specification",
    "text": "Tuning Grid Specification\n\nlr_reg_grid <- tibble(penalty = 10^seq(-4, -1, length.out = 30))\nlr_reg_grid %>% top_n(-5)\n\nSelecting by penalty\n\n\n# A tibble: 5 × 1\n   penalty\n     <dbl>\n1 0.0001  \n2 0.000127\n3 0.000161\n4 0.000204\n5 0.000259\n\nlr_reg_grid %>% top_n(5)\n\nSelecting by penalty\n\n\n# A tibble: 5 × 1\n  penalty\n    <dbl>\n1  0.0386\n2  0.0489\n3  0.0621\n4  0.0788\n5  0.1"
  },
  {
    "objectID": "fluanalysis/code/machinelearning.html#tuning-using-cross-validation-and-the-tune_grid-function",
    "href": "fluanalysis/code/machinelearning.html#tuning-using-cross-validation-and-the-tune_grid-function",
    "title": "My Data Analysis Portfolio",
    "section": "Tuning using cross-validation and the tune_grid() function",
    "text": "Tuning using cross-validation and the tune_grid() function\n\nlr_res <- lr_workflow %>%\n  tune_grid(resamples = folds_train,\n            grid = lr_reg_grid,\n            control = control_grid(verbose = FALSE, save_pred = TRUE),\n            metrics = metric_set(rmse))\n\nlr_res %>% collect_metrics()\n\n# A tibble: 30 × 7\n    penalty .metric .estimator  mean     n std_err .config              \n      <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n 1 0.0001   rmse    standard    1.18    25  0.0167 Preprocessor1_Model01\n 2 0.000127 rmse    standard    1.18    25  0.0167 Preprocessor1_Model02\n 3 0.000161 rmse    standard    1.18    25  0.0167 Preprocessor1_Model03\n 4 0.000204 rmse    standard    1.18    25  0.0167 Preprocessor1_Model04\n 5 0.000259 rmse    standard    1.18    25  0.0167 Preprocessor1_Model05\n 6 0.000329 rmse    standard    1.18    25  0.0167 Preprocessor1_Model06\n 7 0.000418 rmse    standard    1.18    25  0.0167 Preprocessor1_Model07\n 8 0.000530 rmse    standard    1.18    25  0.0167 Preprocessor1_Model08\n 9 0.000672 rmse    standard    1.18    25  0.0167 Preprocessor1_Model09\n10 0.000853 rmse    standard    1.18    25  0.0167 Preprocessor1_Model10\n# … with 20 more rows\n\n\n\n#plot using autoplot\nlr_res %>% autoplot()\n\n\n\n\n\n#getting best model\nlr_res %>%\n  show_best(metric = \"rmse\")\n\n# A tibble: 5 × 7\n  penalty .metric .estimator  mean     n std_err .config              \n    <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n1  0.0621 rmse    standard    1.15    25  0.0169 Preprocessor1_Model28\n2  0.0489 rmse    standard    1.15    25  0.0169 Preprocessor1_Model27\n3  0.0386 rmse    standard    1.15    25  0.0169 Preprocessor1_Model26\n4  0.0788 rmse    standard    1.16    25  0.0171 Preprocessor1_Model29\n5  0.0304 rmse    standard    1.16    25  0.0169 Preprocessor1_Model25\n\nbest_lr <- lr_res %>%\n  select_best(metric = \"rmse\")\nbest_lr\n\n# A tibble: 1 × 2\n  penalty .config              \n    <dbl> <chr>                \n1  0.0621 Preprocessor1_Model28\n\n\n\n#finalizing workflows with best models\nfinal_lr_wf <- lr_workflow %>%\n  finalize_workflow(best_lr)\n\nfinal_lr_fit <- final_lr_wf %>% fit(data=flu_train1)\nfinal_lr_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:  glmnet::glmnet(x = maybe_matrix(x), y = y, family = \"gaussian\",      alpha = ~1) \n\n   Df  %Dev   Lambda\n1   0  0.00 0.271900\n2   2  1.24 0.247700\n3   2  2.67 0.225700\n4   2  3.86 0.205700\n5   2  4.85 0.187400\n6   2  5.67 0.170800\n7   2  6.35 0.155600\n8   2  6.91 0.141800\n9   5  7.57 0.129200\n10  5  8.27 0.117700\n11  8  9.06 0.107200\n12  8  9.81 0.097710\n13  9 10.44 0.089030\n14  9 11.09 0.081120\n15  9 11.63 0.073920\n16 10 12.12 0.067350\n17 10 12.56 0.061370\n18 12 13.00 0.055910\n19 14 13.45 0.050950\n20 16 13.85 0.046420\n21 19 14.24 0.042300\n22 19 14.59 0.038540\n23 19 14.87 0.035120\n24 22 15.17 0.032000\n25 22 15.44 0.029150\n26 22 15.67 0.026560\n27 22 15.85 0.024200\n28 23 16.01 0.022050\n29 24 16.15 0.020090\n30 25 16.28 0.018310\n31 25 16.39 0.016680\n32 25 16.49 0.015200\n33 25 16.56 0.013850\n34 26 16.63 0.012620\n35 27 16.69 0.011500\n36 27 16.73 0.010480\n37 27 16.77 0.009547\n38 27 16.81 0.008698\n39 28 16.84 0.007926\n40 29 16.86 0.007222\n41 29 16.88 0.006580\n42 29 16.90 0.005995\n43 29 16.91 0.005463\n44 29 16.92 0.004978\n45 30 16.93 0.004535\n46 30 16.94 0.004132\n\n...\nand 22 more lines.\n\n\n\nx <- final_lr_fit$fit$fit$fit\nplot(x, \"lambda\")\n\n\n\n\nRandom Forest"
  },
  {
    "objectID": "fluanalysis/code/machinelearning.html#model-specification-2",
    "href": "fluanalysis/code/machinelearning.html#model-specification-2",
    "title": "My Data Analysis Portfolio",
    "section": "Model Specification",
    "text": "Model Specification\n\ncores <- parallel::detectCores()\ncores\n\n[1] 8\n\nrf_mod <-\n  rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>%\n  set_engine(\"ranger\", num.threads = cores) %>%\n  set_mode(\"regression\")"
  },
  {
    "objectID": "fluanalysis/code/machinelearning.html#workflow-definition-2",
    "href": "fluanalysis/code/machinelearning.html#workflow-definition-2",
    "title": "My Data Analysis Portfolio",
    "section": "Workflow Definition",
    "text": "Workflow Definition\n\nrf_workflow <-\n  workflow() %>%\n  add_model(rf_mod) %>%\n  add_recipe(flu_rec)"
  },
  {
    "objectID": "fluanalysis/code/machinelearning.html#tuning-grid-specification-2",
    "href": "fluanalysis/code/machinelearning.html#tuning-grid-specification-2",
    "title": "My Data Analysis Portfolio",
    "section": "Tuning Grid Specification",
    "text": "Tuning Grid Specification\n\nrf_mod\n\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  mtry = tune()\n  trees = 1000\n  min_n = tune()\n\nEngine-Specific Arguments:\n  num.threads = cores\n\nComputational engine: ranger \n\nextract_parameter_set_dials(rf_mod)\n\nCollection of 2 parameters for tuning\n\n identifier  type    object\n       mtry  mtry nparam[?]\n      min_n min_n nparam[+]\n\nModel parameters needing finalization:\n   # Randomly Selected Predictors ('mtry')\n\nSee `?dials::finalize` or `?dials::update.parameters` for more information."
  },
  {
    "objectID": "fluanalysis/code/machinelearning.html#tuning-using-cross-validation-and-the-tune_grid",
    "href": "fluanalysis/code/machinelearning.html#tuning-using-cross-validation-and-the-tune_grid",
    "title": "My Data Analysis Portfolio",
    "section": "Tuning Using Cross-Validation and the Tune_Grid()",
    "text": "Tuning Using Cross-Validation and the Tune_Grid()\n\nrf_res <- rf_workflow %>%\n  tune_grid(resamples = folds_train,\n            grid = 25,\n            control = control_grid(save_pred = TRUE),\n            metrics = metric_set(rmse))\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\nrf_res %>%\n  collect_metrics()\n\n# A tibble: 25 × 8\n    mtry min_n .metric .estimator  mean     n std_err .config              \n   <int> <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n 1    13    13 rmse    standard    1.19    25  0.0165 Preprocessor1_Model01\n 2     5    36 rmse    standard    1.16    25  0.0166 Preprocessor1_Model02\n 3    16    28 rmse    standard    1.18    25  0.0164 Preprocessor1_Model03\n 4    30    40 rmse    standard    1.18    25  0.0167 Preprocessor1_Model04\n 5    11    30 rmse    standard    1.17    25  0.0163 Preprocessor1_Model05\n 6     7    26 rmse    standard    1.17    25  0.0166 Preprocessor1_Model06\n 7    22    26 rmse    standard    1.19    25  0.0167 Preprocessor1_Model07\n 8    10    11 rmse    standard    1.19    25  0.0163 Preprocessor1_Model08\n 9     7     2 rmse    standard    1.19    25  0.0164 Preprocessor1_Model09\n10     9     6 rmse    standard    1.19    25  0.0162 Preprocessor1_Model10\n# … with 15 more rows\n\n\n\n#plot with autoplot\nautoplot(rf_res)\n\n\n\n\n\n#getting best model\nrf_res %>%\n  show_best(metric = \"rmse\")\n\n# A tibble: 5 × 8\n   mtry min_n .metric .estimator  mean     n std_err .config              \n  <int> <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n1     5    36 rmse    standard    1.16    25  0.0166 Preprocessor1_Model02\n2     6    33 rmse    standard    1.17    25  0.0165 Preprocessor1_Model23\n3     7    26 rmse    standard    1.17    25  0.0166 Preprocessor1_Model06\n4     2    12 rmse    standard    1.17    25  0.0167 Preprocessor1_Model25\n5     2     4 rmse    standard    1.17    25  0.0167 Preprocessor1_Model19\n\nbest_rf <- rf_res %>%\n  select_best(metric = \"rmse\")\nbest_rf\n\n# A tibble: 1 × 3\n   mtry min_n .config              \n  <int> <int> <chr>                \n1     5    36 Preprocessor1_Model02\n\n\n\n#finalizing workflow with best model\nfinal_rf_wf <- rf_workflow %>%\n  finalize_workflow(best_rf)\n\nfinal_rf_fit <- final_rf_wf %>% fit(data=flu_train1)\nfinal_rf_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, mtry = min_cols(~5L,      x), num.trees = ~1000, min.node.size = min_rows(~36L, x),      num.threads = ~cores, verbose = FALSE, seed = sample.int(10^5,          1)) \n\nType:                             Regression \nNumber of trees:                  1000 \nSample size:                      508 \nNumber of independent variables:  31 \nMtry:                             5 \nTarget node size:                 36 \nVariable importance mode:         none \nSplitrule:                        variance \nOOB prediction error (MSE):       1.367179 \nR squared (OOB):                  0.06700001"
  },
  {
    "objectID": "fluanalysis/code/machinelearning.html#model-evaluation",
    "href": "fluanalysis/code/machinelearning.html#model-evaluation",
    "title": "My Data Analysis Portfolio",
    "section": "Model Evaluation",
    "text": "Model Evaluation\n\ngraphing actual vs predicted and residuals for each model\n\nTree\n\n# get predicted and residual values in one dataset \n## using augment() instead of predict() here so I can store everything in one df for easier graphing\ntree_predict <- final_tree_fit %>%\n  augment(flu_train1) %>% \n  select(c(.pred, BodyTemp)) %>%\n  mutate(resid = BodyTemp - .pred) \ntree_predict\n\n# A tibble: 508 × 3\n   .pred BodyTemp  resid\n   <dbl>    <dbl>  <dbl>\n 1  99.2     97.8 -1.44 \n 2  99.2     98.1 -1.14 \n 3  98.7     98.1 -0.591\n 4  98.7     98.2 -0.491\n 5  98.7     97.8 -0.891\n 6  98.7     98.2 -0.491\n 7  98.7     98.1 -0.591\n 8  99.2     98   -1.24 \n 9  99.2     97.7 -1.54 \n10  99.2     98.2 -1.04 \n# … with 498 more rows\n\n# Plot actual values vs predicted values\ntree_pred_plot <- tree_predict %>%\n  ggplot(aes(x = BodyTemp, y = .pred)) + \n  geom_point() + \n  labs(title = \"Predictions vs Actual\", \n       x = \"Body Temp Actual\", \n       y = \"Body Temp Prediction\")\ntree_pred_plot\n\n\n\n# Plot pred values vs residuals\ntree_resid_plot <- tree_predict %>% \n  ggplot(aes(x = resid, y = .pred)) + \n  geom_point() +\n  labs(title = \"Predictions vs Residual\", \n       x = \"Body Temp Residual\", \n       y = \"Body Temp Prediction\")\ntree_resid_plot\n\n\n\n\n\n\nLasso\n\n# repeating the above process for lasso\n# get predicted and residual values in one dataset \nlr_predict <- final_lr_fit %>%\n  augment(flu_train1) %>% \n  select(c(.pred, BodyTemp)) %>%\n  mutate(resid = BodyTemp - .pred) \nlr_predict\n\n# A tibble: 508 × 3\n   .pred BodyTemp  resid\n   <dbl>    <dbl>  <dbl>\n 1  98.8     97.8 -0.950\n 2  98.8     98.1 -0.719\n 3  98.5     98.1 -0.360\n 4  98.8     98.2 -0.606\n 5  98.7     97.8 -0.907\n 6  98.7     98.2 -0.523\n 7  98.4     98.1 -0.257\n 8  99.3     98   -1.26 \n 9  98.9     97.7 -1.24 \n10  99.0     98.2 -0.769\n# … with 498 more rows\n\n# Plot actual values vs predicted values\nlr_pred_plot <- lr_predict %>%\n  ggplot(aes(x = BodyTemp, y = .pred)) + \n  geom_point() + \n  labs(title = \"Predictions vs Actual\", \n       x = \"Body Temp Actual\", \n       y = \"Body Temp Prediction\")\nlr_pred_plot\n\n\n\n# Plot pred values vs residuals\nlr_resid_plot <- lr_predict %>% \n  ggplot(aes(x = resid, y = .pred)) + \n  geom_point() +\n  labs(title = \"Predictions vs Residual\", \n       x = \"Body Temp Residual\", \n       y = \"Body Temp Prediction\")\nlr_resid_plot\n\n\n\n\n\n\nRandom forest\n\n# repeating again for random forest\n# get predicted and residual values in one dataset \nrf_predict <- final_rf_fit %>%\n  augment(flu_train1) %>% \n  select(c(.pred, BodyTemp)) %>%\n  mutate(resid = BodyTemp - .pred) \nrf_predict\n\n# A tibble: 508 × 3\n   .pred BodyTemp  resid\n   <dbl>    <dbl>  <dbl>\n 1  98.7     97.8 -0.929\n 2  98.6     98.1 -0.487\n 3  98.7     98.1 -0.595\n 4  98.7     98.2 -0.535\n 5  98.8     97.8 -0.998\n 6  98.6     98.2 -0.394\n 7  98.4     98.1 -0.256\n 8  99.1     98   -1.06 \n 9  98.8     97.7 -1.06 \n10  98.9     98.2 -0.673\n# … with 498 more rows\n\n# Plot actual values vs predicted values\nrf_pred_plot <- rf_predict %>%\n  ggplot(aes(x = BodyTemp, y = .pred)) + \n  geom_point() + \n  labs(title = \"Predictions vs Actual\", \n       x = \"Body Temp Actual\", \n       y = \"Body Temp Prediction\")\nrf_pred_plot\n\n\n\n# Plot pred values vs residuals\nrf_resid_plot <- rf_predict %>% \n  ggplot(aes(x = resid, y = .pred)) + \n  geom_point() +\n  labs(title = \"Predictions vs Residual\", \n       x = \"Body Temp Residual\", \n       y = \"Body Temp Prediction\")\nrf_resid_plot\n\n\n\n\n\n\n\nviewing performance\n\n#comparing best models to null model to determine which model performed the best\ntree_res %>%\n  show_best(metric = \"rmse\", n=1)\n\n# A tibble: 1 × 8\n  cost_complexity tree_depth .metric .estimator  mean     n std_err .config     \n            <dbl>      <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>       \n1    0.0000000001          1 rmse    standard    1.19    25  0.0181 Preprocesso…\n\nlr_res %>%\n  show_best(metric = \"rmse\", n=1)\n\n# A tibble: 1 × 7\n  penalty .metric .estimator  mean     n std_err .config              \n    <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n1  0.0621 rmse    standard    1.15    25  0.0169 Preprocessor1_Model28\n\nrf_res %>%\n  show_best(metric = \"rmse\", n=1)\n\n# A tibble: 1 × 8\n   mtry min_n .metric .estimator  mean     n std_err .config              \n  <int> <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n1     5    36 rmse    standard    1.16    25  0.0166 Preprocessor1_Model02\n\nnull_train_fit %>% \n  collect_metrics(metric = \"rmse\")\n\n# A tibble: 2 × 6\n  .metric .estimator   mean     n std_err .config             \n  <chr>   <chr>       <dbl> <int>   <dbl> <chr>               \n1 rmse    standard     1.21    25  0.0177 Preprocessor1_Model1\n2 rsq     standard   NaN        0 NA      Preprocessor1_Model1\n\n\n\nAfter evaluating all the models it appears that LASSO preformed the best as it has the lowest RMSE at 1.15 so I will be using it as my final model.\nFinal evaluation\n\n#fitting lasso model to testing data with last_fit()\nlr_last_fit <- final_lr_wf %>%\n  last_fit(split)\n\nlr_last_fit %>% collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard      1.15   Preprocessor1_Model1\n2 rsq     standard      0.0291 Preprocessor1_Model1\n\n#includign null test metric for comparison\nnull_test_fit %>% collect_metrics()\n\n# A tibble: 2 × 6\n  .metric .estimator   mean     n std_err .config             \n  <chr>   <chr>       <dbl> <int>   <dbl> <chr>               \n1 rmse    standard     1.16    25  0.0285 Preprocessor1_Model1\n2 rsq     standard   NaN        0 NA      Preprocessor1_Model1\n\n\n\n\nI attempted the graphs but was throwing major errors trying to predict() or augment() based on a last_fit object. If I have time I will attempt this again but it will likely be later. Maybe someone else was successful and could provide some pointers?\nThe last fit returned an RMSE of 1.15341145 which is slightly higher than the best LASSO model above but better than the null model ran against the testing data."
  },
  {
    "objectID": "fluanalysis/code/modeleval.html",
    "href": "fluanalysis/code/modeleval.html",
    "title": "My Data Analysis Portfolio",
    "section": "",
    "text": "Model Evaluation\n\nLoading packages and cleaned data\n\n\n# load packages\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.0     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.1     ✔ tibble    3.1.8\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nlibrary(here)\n\nhere() starts at C:/Users/Katie/Documents/2022-2023/MADA/katiewells-MADA-portfolio\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.0.0 ──\n✔ broom        1.0.3     ✔ rsample      1.1.1\n✔ dials        1.1.0     ✔ tune         1.0.1\n✔ infer        1.0.4     ✔ workflows    1.1.3\n✔ modeldata    1.1.0     ✔ workflowsets 1.0.0\n✔ parsnip      1.0.4     ✔ yardstick    1.1.0\n✔ recipes      1.0.5     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Dig deeper into tidy modeling with R at https://www.tmwr.org\n\n\n\n# load cleaned data\nflu2 <- readRDS(here(\"fluanalysis\", \"data\", \"flu2.rds\"))\n\n\nSplit data\n\n\nset.seed(222)\n# Put 3/4 of the data into the training set \ndata_split <- initial_split(flu2, prop = 3/4)\n\n# Create data frames for the two sets:\ntrain_data <- training(data_split)\ntest_data  <- testing(data_split)\n\n\nCreate recipe\n\n\nflu_rec <- \n  recipe(Nausea ~ ., data = train_data) \n\n\nFit model with recipe\n\n\n#setting logistic regression engine\nlr_mod <- \n  logistic_reg() %>% \n  set_engine(\"glm\")\n\n\n#creating workflow using model and recipe\nflu_wflow <- \n  workflow() %>% \n  add_model(lr_mod) %>% \n  add_recipe(flu_rec)\nflu_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm \n\n\n\n#make function to prepare the recipe and train the model \nflu_fit <- \n  flu_wflow %>% \n  fit(data = train_data)\n\n\n#pull fitting model object and make tibble of model coefficients\nflu_fit %>% \n  extract_fit_parsnip() %>% \n  tidy()\n\n# A tibble: 32 × 5\n   term                 estimate std.error statistic p.value\n   <chr>                   <dbl>     <dbl>     <dbl>   <dbl>\n 1 (Intercept)            2.56       9.39      0.273  0.785 \n 2 SwollenLymphNodesYes  -0.240      0.231    -1.04   0.300 \n 3 ChestCongestionYes     0.168      0.252     0.668  0.504 \n 4 ChillsSweatsYes        0.148      0.331     0.448  0.654 \n 5 NasalCongestionYes     0.600      0.309     1.94   0.0521\n 6 SneezeYes              0.0974     0.247     0.395  0.693 \n 7 FatigueYes             0.180      0.438     0.411  0.681 \n 8 SubjectiveFeverYes     0.191      0.261     0.733  0.463 \n 9 HeadacheYes            0.482      0.351     1.37   0.169 \n10 Weakness.L             0.668      0.406     1.65   0.0999\n# … with 22 more rows\n\n\n\nUsing trained workflow to predict (test data)\n\n\n#use trained model to predict with unseen data\npredict(flu_fit, test_data)\n\n# A tibble: 183 × 1\n   .pred_class\n   <fct>      \n 1 No         \n 2 No         \n 3 No         \n 4 No         \n 5 No         \n 6 Yes        \n 7 Yes        \n 8 No         \n 9 No         \n10 Yes        \n# … with 173 more rows\n\n\n\n#save above together to use to get ROC\nflu_aug <- \n  augment(flu_fit, test_data)\n\n\n#view the data\nflu_aug %>%\n  select(Nausea, .pred_No, .pred_Yes)\n\n# A tibble: 183 × 3\n   Nausea .pred_No .pred_Yes\n   <fct>     <dbl>     <dbl>\n 1 No        0.962    0.0377\n 2 Yes       0.708    0.292 \n 3 No        0.696    0.304 \n 4 Yes       0.548    0.452 \n 5 No        0.826    0.174 \n 6 Yes       0.193    0.807 \n 7 Yes       0.227    0.773 \n 8 No        0.712    0.288 \n 9 No        0.688    0.312 \n10 Yes       0.271    0.729 \n# … with 173 more rows\n\n\n\n#create ROC curve with predicted class probabilities \nflu_aug %>% \n  roc_curve(truth = Nausea, .pred_No) %>% \n  autoplot()\n\nWarning: Returning more (or less) than 1 row per `summarise()` group was deprecated in\ndplyr 1.1.0.\nℹ Please use `reframe()` instead.\nℹ When switching from `summarise()` to `reframe()`, remember that `reframe()`\n  always returns an ungrouped data frame and adjust accordingly.\nℹ The deprecated feature was likely used in the yardstick package.\n  Please report the issue at <https://github.com/tidymodels/yardstick/issues>.\n\n\n\n\n\n\n#ROC estimate\nflu_aug %>% \n  roc_auc(truth = Nausea, .pred_No) \n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.731\n\n\nRefitting with the main predicotr (RunnyNose) and using the trained model to predict with test and train data.\n\nAlternative Model\n\n\nflu_rec2 <- \n  recipe(Nausea ~ RunnyNose, data = train_data) \n\n\nflu_wflow2 <- \n  workflow() %>% \n  add_model(lr_mod) %>% \n  add_recipe(flu_rec2)\nflu_wflow2\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm \n\n\n\nflu_fit2 <- \n  flu_wflow2 %>% \n  fit(data = train_data)\n\n\nflu_fit2 %>% \n  extract_fit_parsnip() %>% \n  tidy()\n\n# A tibble: 2 × 5\n  term         estimate std.error statistic    p.value\n  <chr>           <dbl>     <dbl>     <dbl>      <dbl>\n1 (Intercept)    -0.790     0.172    -4.59  0.00000447\n2 RunnyNoseYes    0.188     0.202     0.930 0.352     \n\n\n\nUsing trained workflow to predict (test data)\n\n\npredict(flu_fit2, test_data)\n\n# A tibble: 183 × 1\n   .pred_class\n   <fct>      \n 1 No         \n 2 No         \n 3 No         \n 4 No         \n 5 No         \n 6 No         \n 7 No         \n 8 No         \n 9 No         \n10 No         \n# … with 173 more rows\n\n\n\nflu_aug3 <- \n  augment(flu_fit2, test_data)\n\n\nflu_aug3 %>%\n  select(Nausea, .pred_No, .pred_Yes)\n\n# A tibble: 183 × 3\n   Nausea .pred_No .pred_Yes\n   <fct>     <dbl>     <dbl>\n 1 No        0.688     0.312\n 2 Yes       0.646     0.354\n 3 No        0.646     0.354\n 4 Yes       0.646     0.354\n 5 No        0.688     0.312\n 6 Yes       0.688     0.312\n 7 Yes       0.646     0.354\n 8 No        0.688     0.312\n 9 No        0.688     0.312\n10 Yes       0.688     0.312\n# … with 173 more rows\n\n\n\nflu_aug3 %>% \n  roc_curve(truth = Nausea, .pred_No) %>% \n  autoplot()\n\n\n\n\n\nflu_aug3 %>% \n  roc_auc(truth = Nausea, .pred_No) \n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.466\n\n\nThe model using only runny nose as a predictor had a lower ROC than the model with all predictors.\n\nThis section added by Hayley Hemme\n\nCreate recipe\n\n\nflu_ln_rec <- \n  recipe(BodyTemp ~ ., data = train_data) \n\n\nFit linear model\n\n\n#setting linear regression engine\nln_mod <- \n  linear_reg() %>% \n  set_engine(\"lm\") %>% \n  set_mode(\"regression\")\n\n\n#creating workflow using model and recipe\nflu_ln_wflow <- \n  workflow() %>% \n  add_model(ln_mod) %>% \n  add_recipe(flu_ln_rec)\n\n\n#make function to prepare the recipe and train the model \nflu_ln_fit <- \n  flu_ln_wflow %>% \n  fit(data = train_data)\n\n\n#pull fitting model object and make tibble of model coefficients\nflu_ln_fit %>% \n  extract_fit_parsnip() %>% \n  tidy()\n\n# A tibble: 32 × 5\n   term                 estimate std.error statistic    p.value\n   <chr>                   <dbl>     <dbl>     <dbl>      <dbl>\n 1 (Intercept)           98.0        0.280   350.    0         \n 2 SwollenLymphNodesYes  -0.190      0.108    -1.76  0.0789    \n 3 ChestCongestionYes     0.146      0.115     1.27  0.203     \n 4 ChillsSweatsYes        0.184      0.148     1.24  0.214     \n 5 NasalCongestionYes    -0.182      0.136    -1.34  0.180     \n 6 SneezeYes             -0.474      0.113    -4.18  0.0000338 \n 7 FatigueYes             0.362      0.187     1.94  0.0529    \n 8 SubjectiveFeverYes     0.564      0.118     4.79  0.00000223\n 9 HeadacheYes            0.0675     0.150     0.448 0.654     \n10 Weakness.L             0.224      0.182     1.23  0.218     \n# … with 22 more rows\n\n\n\nUsing trained workflow to predict (test data)\n\n\n#use trained model to predict with unseen data\npredict(flu_ln_fit, test_data)\n\n# A tibble: 183 × 1\n   .pred\n   <dbl>\n 1  99.4\n 2  99.0\n 3  99.8\n 4  98.7\n 5  99.0\n 6  99.6\n 7  99.2\n 8  98.8\n 9  99.5\n10  98.8\n# … with 173 more rows\n\n\n\n#save above together \nflu_ln_aug <- \n  augment(flu_ln_fit, test_data)\n\n\n#view the data\nflu_ln_aug %>%\n  select(BodyTemp, .pred)\n\n# A tibble: 183 × 2\n   BodyTemp .pred\n      <dbl> <dbl>\n 1     98.3  99.4\n 2     98.8  99.0\n 3    102.   99.8\n 4     98.2  98.7\n 5     97.8  99.0\n 6     97.8  99.6\n 7    100    99.2\n 8    101.   98.8\n 9     98.8  99.5\n10    100.   98.8\n# … with 173 more rows\n\n\nMeasuring model fit - RMSE\n\nflu_ln_aug %>%\n  rmse(truth = BodyTemp, .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        1.15\n\n\nLet’s also measure model fit using R2\n\nflu_ln_aug %>% \n  rsq(truth = BodyTemp, .pred) \n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rsq     standard      0.0446\n\n\nMaking a plot using predictions\n\nggplot(flu_ln_aug, aes(x = BodyTemp, y = .pred)) +\n  geom_point(alpha = 0.5) + \n  geom_abline(color = 'blue', linetype = 2) +\n  coord_obs_pred() +\n  labs(x = 'Actual Body Temperature', y = 'Predicted Body Temperature')\n\n\n\n\nThis trained model is not very well suited to predict body temperature using all the predictors in the model. Let move on and…\nRefit with the main predictor (RunnyNose) and use the trained model to predict with test and train data. 11. Alternative Model Creating recipe\n\nflu_ln_rec2 <- \n  recipe(BodyTemp ~ RunnyNose, data = train_data) \n\nMaking workflow object\n\nflu_ln_wflow2 <- \n  workflow() %>% \n  add_model(ln_mod) %>% \n  add_recipe(flu_ln_rec2)\n\nMaking fit object\n\nflu_ln_fit2 <- \n  flu_ln_wflow2 %>% \n  fit(data = train_data)\n\nExtracting data of the model fit\n\nflu_ln_fit2 %>% \n  extract_fit_parsnip() %>% \n  tidy()\n\n# A tibble: 2 × 5\n  term         estimate std.error statistic p.value\n  <chr>           <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)    99.1      0.0964   1028.    0     \n2 RunnyNoseYes   -0.261    0.114      -2.29  0.0225\n\n\n\nUsing trained workflow to predict (test data)\n\n\npredict(flu_ln_fit2, test_data)\n\n# A tibble: 183 × 1\n   .pred\n   <dbl>\n 1  99.1\n 2  98.9\n 3  98.9\n 4  98.9\n 5  99.1\n 6  99.1\n 7  98.9\n 8  99.1\n 9  99.1\n10  99.1\n# … with 173 more rows\n\n\n\nflu_ln_aug3 <- \n  augment(flu_ln_fit2, test_data)\n\n\nflu_ln_aug3 %>%\n  select(BodyTemp, .pred)\n\n# A tibble: 183 × 2\n   BodyTemp .pred\n      <dbl> <dbl>\n 1     98.3  99.1\n 2     98.8  98.9\n 3    102.   98.9\n 4     98.2  98.9\n 5     97.8  99.1\n 6     97.8  99.1\n 7    100    98.9\n 8    101.   99.1\n 9     98.8  99.1\n10    100.   99.1\n# … with 173 more rows\n\n\nMeasuring model fit - RMSE\n\nflu_ln_aug3 %>%\n  rmse(truth = BodyTemp, .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        1.13\n\n\nLet’s also measure model fit using R2\n\nflu_ln_aug3 %>% \n  rsq(truth = BodyTemp, .pred) \n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rsq     standard      0.0240\n\n\nMake an R2 plot using predictions\n\nggplot(flu_ln_aug3, aes(x = BodyTemp, y = .pred)) +\n  geom_point(alpha = 0.5) + \n  geom_abline(color = 'blue', linetype = 2) +\n  coord_obs_pred() +\n  labs(x = 'Actual Body Temperature', y = 'Predicted Body Temperature')\n\n\n\n\nThe model preformed very poorly using runny nose as predictor of body temperature. I am also wondering why the model only had two different predictions, and makes me think that something may have went wrong with setting up the model?"
  },
  {
    "objectID": "fluanalysis/code/wrangling.html",
    "href": "fluanalysis/code/wrangling.html",
    "title": "My Data Analysis Portfolio",
    "section": "",
    "text": "Wrangling\nLoading the needed packages and the raw data.\nRemoving all variables that have: Score, Total, FluA, FluB, Dxname, or Activity in their name; also removing Unique.Visit and any NA observations.\nSaving the cleaned data in an RDS file."
  },
  {
    "objectID": "fluanalysis/code/wrangling.html#preprocessing-for-module-11",
    "href": "fluanalysis/code/wrangling.html#preprocessing-for-module-11",
    "title": "My Data Analysis Portfolio",
    "section": "Preprocessing for Module 11",
    "text": "Preprocessing for Module 11\n\n#load data\nflu2 <- readRDS(here(\"fluanalysis\", \"data\", \"flu2.rds\"))\n\n\n## Feature/Variable Removal (Weakness, Cough (2x), Myalgia - Yes/No)\nflu2 <- flu2 %>%\n  select(-c(WeaknessYN, CoughYN, CoughYN2, MyalgiaYN))\n\n## Recipe Creation\ncategorical_recipe <- recipe(~ SwollenLymphNodes + ChestCongestion + ChillsSweats + NasalCongestion + Sneeze + Fatigue + SubjectiveFever + Headache + RunnyNose + AbPain + ChestPain + Diarrhea + EyePn + Insomnia + ItchyEye + Nausea + EarPn + Pharyngitis + Breathless + ToothPn + Vomit + Wheeze, data = flu2)\ncategorical_dummies <- categorical_recipe %>%\n  step_dummy(all_predictors()) %>%\n  prep(training = flu2)\ncategorical_dummies_data <- bake(categorical_dummies, new_data = NULL)\n## Recipe Creation\nord_levels <- c(\"None\", \"Mild\", \"Moderate\", \"Severe\")\nflu2 <- flu2 %>%\n  mutate(Weakness = ordered(Weakness),\n         CoughIntensity = ordered(CoughIntensity),\n         Myalgia = ordered(Myalgia))\nord_recipe <- recipe(~ Weakness + CoughIntensity + Myalgia, data = flu2)\nord_dummies <- ord_recipe %>%\n  step_ordinalscore(all_predictors()) %>%\n  prep(training = flu2)\nord_dummies_data <- bake(ord_dummies, new_data = NULL)\n\n## Low (\"near-zero\") variance predictors\nxtabs(~ Vision + Hearing, data = flu2)\n\n      Hearing\nVision  No Yes\n   No  684  27\n   Yes  16   3\n\n### Less than 50 observations where Hearing and Vision == \"Yes\"\nflu2 <- flu2 %>%\n  select(-c(Vision, Hearing))\n\n# Save cleaned set\nsaveRDS(flu2, file = here(\"fluanalysis\", \"data\", \"flu2.rds\"))"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MADA website and data analysis portfolio",
    "section": "",
    "text": "Hey y’all!\n\n:)\nWelcome to my website and data analysis portfolio.\n\nPlease use the Menu Bar above to look around.\nHave fun and GO DAWGS!"
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "MADA Project",
    "section": "",
    "text": "When it’s finished, I’ll add the high points of my MADA project (in collaboration with my good friend Connor Ross) here so it can be viewed from my website."
  },
  {
    "objectID": "tidytuesday_exercise.html",
    "href": "tidytuesday_exercise.html",
    "title": "Tidy Tuesday Exercise",
    "section": "",
    "text": "Here’s my contribution to the Tidy Tuesday analysis for the 2/14/23 dataset Hollywood Age Gaps. This data was taken from Hollywood Age Gap via https://www.data-is-plural.com/archive/2018-02-07-edition/. It takes a look at the differences between ages in couples in movies. Some things I would like to explore are:\n1) the relationship between release year and age differences\n2) how actor/character age and gender are related\n3) how much the movies deviate from the average age difference\nLet’s load the packages I’ll need.\n\n#load packages\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.0     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.1     ✔ tibble    3.1.8\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nlibrary(ggplot2)\nlibrary(plotly)\n\n\nAttaching package: 'plotly'\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\nThe following object is masked from 'package:stats':\n\n    filter\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\n\nNow for the data; let’s load it and take a look at the structure.\n\n#load data\nage_gaps <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-02-14/age_gaps.csv')\n\nRows: 1155 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (6): movie_name, director, actor_1_name, actor_2_name, character_1_gend...\ndbl  (5): release_year, age_difference, couple_number, actor_1_age, actor_2_age\ndate (2): actor_1_birthdate, actor_2_birthdate\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nglimpse(age_gaps)\n\nRows: 1,155\nColumns: 13\n$ movie_name         <chr> \"Harold and Maude\", \"Venus\", \"The Quiet American\", …\n$ release_year       <dbl> 1971, 2006, 2002, 1998, 2010, 1992, 2009, 1999, 199…\n$ director           <chr> \"Hal Ashby\", \"Roger Michell\", \"Phillip Noyce\", \"Joe…\n$ age_difference     <dbl> 52, 50, 49, 45, 43, 42, 40, 39, 38, 38, 36, 36, 35,…\n$ couple_number      <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ actor_1_name       <chr> \"Ruth Gordon\", \"Peter O'Toole\", \"Michael Caine\", \"D…\n$ actor_2_name       <chr> \"Bud Cort\", \"Jodie Whittaker\", \"Do Thi Hai Yen\", \"T…\n$ character_1_gender <chr> \"woman\", \"man\", \"man\", \"man\", \"man\", \"man\", \"man\", …\n$ character_2_gender <chr> \"man\", \"woman\", \"woman\", \"woman\", \"man\", \"woman\", \"…\n$ actor_1_birthdate  <date> 1896-10-30, 1932-08-02, 1933-03-14, 1930-09-17, 19…\n$ actor_2_birthdate  <date> 1948-03-29, 1982-06-03, 1982-10-01, 1975-11-08, 19…\n$ actor_1_age        <dbl> 75, 74, 69, 68, 81, 59, 62, 69, 57, 77, 59, 56, 65,…\n$ actor_2_age        <dbl> 23, 24, 20, 23, 38, 17, 22, 30, 19, 39, 23, 20, 30,…\n\n\nThis data has been relatively cleaned and wrangled already; let’s just get some stats I might be interested in.\n\nage_gaps %>% pull(release_year) %>% range()\n\n[1] 1935 2022\n\nage_gaps %>% pull(actor_1_age) %>% range()\n\n[1] 18 81\n\nage_gaps %>% pull(actor_2_age) %>% range()\n\n[1] 17 68\n\nage_gaps %>% pull(age_difference) %>% range()\n\n[1]  0 52\n\n\nThis gives me the range of release years, actor ages, and age differences for all the movies. Seeing these, let’s plot the release years against age differences and see if any trends appear. Just for fun, let’s make this interactive so we can see which movies fall where. This will let me hover over each dot and see the movie name and age difference. I can do this with plotly.\n\nplot <- age_gaps %>% ggplot(aes(x=release_year, y=age_difference)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") + \n  labs(title=\"Age Gaps in Movies through the Years\", x=\"Year\", y=\"Age Difference\")\nggplotly(plot)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\nWell…there seems to be a slight negative correlation between release year and age difference, but more movies have been released in recent years which is likely skewing the data. The interactive portion provided by plotly() makes it very easy to see the movie with the largest age difference (52 years) was released in 1971.\nLet’s look at the frequency of age differences with a bar graph.\n\nage_gaps %>% ggplot(aes(x=age_difference)) + \n  geom_bar(fill=\"#00C5CD\") + \n  scale_x_continuous(n.breaks=10) + \n  labs(title=\"Age Gap Frequency\", x=\"Age difference (years)\", y=\"observations\")\n\n\n\n\nLooks like 2 and 3 years is the most popular age gap in these movies.\nAfter examining the data further, I noticed that actor 1 (and therefore character 1) was always the older of the couple. I want to see the distribution of age difference based on whether the older character is male or female. I think a boxplot would be a good way to visualize this information.\n\n#make a boxplot of character 1 gender vs. age difference\nage_gaps %>% ggplot(aes(x=character_1_gender, y=age_difference, fill=character_1_gender)) + \n  geom_boxplot() + \n  stat_summary(fun = \"mean\", geom = \"point\", shape = 8, size = 2, color = \"black\") + \n  labs(x=\"Older Character Gender\", y=\"Age Difference (years)\") + \n  ggtitle(\"Age Difference and Older Character's Gender\")\n\n\n\n\nThis tells us there are many more movies where the older character is male and that the mean and median age difference is higher in those movies. The one very obvious outlier of Harold and Maude on the older female character side is visible here.\nLet’s see how a random sample of movies compares to the average age difference of this dataset.\n\n#calculate the mean age difference\nage_gaps %>% pull(age_difference) %>% mean()\n\n[1] 10.42424\n\n\n\n#take a (fixed) random sample (so y'all can reproduce if you want)\nset.seed(20)\nrand_age <- age_gaps %>% sample_n(size=20)\n\n\n#make a new column with the z score (normalized age difference)\nrand_age <- rand_age %>% mutate(diff_z = round((age_difference - mean(age_difference))/sd(age_difference), 2))  \n#make a new column telling whether each movie is above or below the average\nrand_age <- rand_age %>% mutate(diff_v = ifelse(diff_z < 0, \"below\", \"above\"))\n\nI wanted a way to show how different these movies were from each other (and the average) in terms of age difference, and a StackOverflow post gave me the idea to do a diverging bar graph.\n\n#graph a diverging bar plot showing how many standard deviations each movie is away from the mean\nggplot(rand_age, aes(x=movie_name, y=diff_z, label=diff_z)) + \n  geom_bar(stat='identity', aes(fill=diff_v), width=.5)  +\n  scale_fill_manual(name=\"Age Difference\", \n                    labels = c(\"Above Average\", \"Below Average\"), \n                    values = c(\"above\"=\"#00C5CD\", \"below\"=\"#f8766d\")) + \n  labs(title= \"Movies & their deviations from the average age difference\", x=\"Movie Name\", y=\"Standard Deviation\") + \n  coord_flip()\n\n\n\n\nSeems like most of the movies in this random sample fall below the average of 10.42424 years between the actors.\nJust for fun, let’s see how many movies in the whole dataset are above or below the average. There’s no way to be able to see all of them in a graph like the one above so let’s just look at the numbers.\n\n#make a new column with the z score (normalized age difference)\nage_gaps <- age_gaps %>% mutate(diff_z = round((age_difference - mean(age_difference))/sd(age_difference), 2))  \n#make a new column telling whether each movie is above or below the average\nage_gaps <- age_gaps %>% mutate(diff_v = ifelse(diff_z < 0, \"below\", \"above\"))\n#get counts of above and below the average\nage_gaps %>% pull(diff_v) %>% table()\n\n.\nabove below \n  464   691 \n\n\nLooks like a 60/40 split.\nOverall it looks like: age gaps have decreased over the years (but movie release has increased), 2-3 years is the most common age gap, men are usually the older actor/character, and only around 40% of movies in this dataset have an above average age difference."
  },
  {
    "objectID": "tidytuesday_exercise2.html",
    "href": "tidytuesday_exercise2.html",
    "title": "Tidy Tuesday Exercise 2",
    "section": "",
    "text": "Here is my contribution for this week’s Tidy Tuesday analysis. The data this week comes from The Humane League’s US Egg Production dataset by Samara Mendez. Dataset and code is available for this project on OSF at US Egg Production Data Set. This dataset tracks the supply of cage-free eggs in the United States from December 2007 to February 2021. Here’s the link to the Tidy Tuesday repository: https://github.com/rfordatascience/tidytuesday/blob/master/data/2023/2023-04-11/readme.md\nLet’s load the packages I’ll need\nLoad in the data\nMain question:\nDoes type of production affect number of eggs produced? Outcome: number of eggs Predictors: number of hens, production process Let’s make a new dataset with only the variables I need.\nMake a recipe for use in all models\nnull with training data\nnull with testing data\nModel Tuning and Fitting\nTree"
  },
  {
    "objectID": "tidytuesday_exercise2.html#model-specification",
    "href": "tidytuesday_exercise2.html#model-specification",
    "title": "Tidy Tuesday Exercise 2",
    "section": "Model Specification",
    "text": "Model Specification\n\ntune_spec <- decision_tree(cost_complexity = tune(),\n                           tree_depth = tune()) %>%\n  set_engine(\"rpart\") %>%\n  set_mode(\"regression\")\ntune_spec\n\nDecision Tree Model Specification (regression)\n\nMain Arguments:\n  cost_complexity = tune()\n  tree_depth = tune()\n\nComputational engine: rpart"
  },
  {
    "objectID": "tidytuesday_exercise2.html#workflow-definition",
    "href": "tidytuesday_exercise2.html#workflow-definition",
    "title": "Tidy Tuesday Exercise 2",
    "section": "Workflow Definition",
    "text": "Workflow Definition\n\ntree_wf <- workflow() %>%\n  add_model(tune_spec) %>%\n  add_recipe(egg_rec)"
  },
  {
    "objectID": "tidytuesday_exercise2.html#tuning-grid-specification",
    "href": "tidytuesday_exercise2.html#tuning-grid-specification",
    "title": "Tidy Tuesday Exercise 2",
    "section": "Tuning Grid Specification",
    "text": "Tuning Grid Specification\n\ntree_grid <- grid_regular(cost_complexity(),\n                          tree_depth(),\n                          levels = 5)\ntree_grid\n\n# A tibble: 25 × 2\n   cost_complexity tree_depth\n             <dbl>      <int>\n 1    0.0000000001          1\n 2    0.0000000178          1\n 3    0.00000316            1\n 4    0.000562              1\n 5    0.1                   1\n 6    0.0000000001          4\n 7    0.0000000178          4\n 8    0.00000316            4\n 9    0.000562              4\n10    0.1                   4\n# … with 15 more rows\n\ntree_grid %>%\n  count(tree_depth)\n\n# A tibble: 5 × 2\n  tree_depth     n\n       <int> <int>\n1          1     5\n2          4     5\n3          8     5\n4         11     5\n5         15     5"
  },
  {
    "objectID": "tidytuesday_exercise2.html#tuning-using-cross-validation-and-tune_grid",
    "href": "tidytuesday_exercise2.html#tuning-using-cross-validation-and-tune_grid",
    "title": "Tidy Tuesday Exercise 2",
    "section": "Tuning using Cross-Validation and Tune_grid()",
    "text": "Tuning using Cross-Validation and Tune_grid()\n\ntree_res <- tree_wf %>%\n  tune_grid(\n    resamples = folds_train,\n    grid = tree_grid\n  )\ntree_res\n\n# Tuning results\n# 2-fold cross-validation repeated 2 times using stratification \n# A tibble: 4 × 5\n  splits          id      id2   .metrics          .notes          \n  <list>          <chr>   <chr> <list>            <list>          \n1 <split [76/76]> Repeat1 Fold1 <tibble [50 × 6]> <tibble [0 × 3]>\n2 <split [76/76]> Repeat1 Fold2 <tibble [50 × 6]> <tibble [0 × 3]>\n3 <split [76/76]> Repeat2 Fold1 <tibble [50 × 6]> <tibble [0 × 3]>\n4 <split [76/76]> Repeat2 Fold2 <tibble [50 × 6]> <tibble [0 × 3]>\n\ntree_res %>%\n  collect_metrics()\n\n# A tibble: 50 × 8\n   cost_complexity tree_depth .metric .estimator      mean     n std_err .config\n             <dbl>      <int> <chr>   <chr>          <dbl> <int>   <dbl> <chr>  \n 1    0.0000000001          1 rmse    standard     3.99e+8     4 5.31e+6 Prepro…\n 2    0.0000000001          1 rsq     standard     9.83e-1     4 4.86e-4 Prepro…\n 3    0.0000000178          1 rmse    standard     3.99e+8     4 5.31e+6 Prepro…\n 4    0.0000000178          1 rsq     standard     9.83e-1     4 4.86e-4 Prepro…\n 5    0.00000316            1 rmse    standard     3.99e+8     4 5.31e+6 Prepro…\n 6    0.00000316            1 rsq     standard     9.83e-1     4 4.86e-4 Prepro…\n 7    0.000562              1 rmse    standard     3.99e+8     4 5.31e+6 Prepro…\n 8    0.000562              1 rsq     standard     9.83e-1     4 4.86e-4 Prepro…\n 9    0.1                   1 rmse    standard     3.99e+8     4 5.31e+6 Prepro…\n10    0.1                   1 rsq     standard     9.83e-1     4 4.86e-4 Prepro…\n# … with 40 more rows\n\n\n\n#plot using autoplot\ntree_res %>% autoplot()\n\n\n\n\n\n#getting best model\ntree_res %>%\n  show_best(metric = \"rmse\")\n\n# A tibble: 5 × 8\n  cost_complexity tree_depth .metric .estimator       mean     n std_err .config\n            <dbl>      <int> <chr>   <chr>           <dbl> <int>   <dbl> <chr>  \n1    0.0000000001          8 rmse    standard   196540838.     4  2.07e7 Prepro…\n2    0.0000000178          8 rmse    standard   196540838.     4  2.07e7 Prepro…\n3    0.00000316            8 rmse    standard   196540838.     4  2.07e7 Prepro…\n4    0.0000000001         11 rmse    standard   196540838.     4  2.07e7 Prepro…\n5    0.0000000178         11 rmse    standard   196540838.     4  2.07e7 Prepro…\n\nbest_tree <- tree_res %>%\n  select_best(metric = \"rmse\")\nbest_tree\n\n# A tibble: 1 × 3\n  cost_complexity tree_depth .config              \n            <dbl>      <int> <chr>                \n1    0.0000000001          8 Preprocessor1_Model11\n\n\n\n#finalizing workflows with best models\nfinal_tree_wf <- tree_wf %>%\n  finalize_workflow(best_tree)\n\nfinal_tree_fit <- final_tree_wf %>% fit(data=egg_train)\nfinal_tree_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_dummy()\n• step_ordinalscore()\n\n── Model ───────────────────────────────────────────────────────────────────────\nn= 152 \n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n 1) root 152 1.422721e+21 2606838000  \n   2) n_hens< 1.82312e+08 114 1.959310e+19  855554500  \n     4) n_hens< 3.41e+07 48 6.160151e+17  404576200  \n       8) n_hens< 2.032325e+07 39 3.084503e+16  354144400  \n        16) n_hens< 1.528325e+07 17 3.404879e+15  329754800 *\n        17) n_hens>=1.528325e+07 22 9.513458e+15  372990900  \n          34) n_hens< 1.579525e+07 14 6.747782e+14  360357000 *\n          35) n_hens>=1.579525e+07 8 2.693482e+15  395100300 *\n       9) n_hens>=2.032325e+07 9 5.614982e+16  623114000 *\n     5) n_hens>=3.41e+07 66 2.114922e+18 1183539000  \n      10) n_hens< 4.4231e+07 10 3.954328e+16  899601600 *\n      11) n_hens>=4.4231e+07 56 1.125212e+18 1234242000  \n        22) prod_process>=2 38 1.266231e+17 1170392000  \n          44) n_hens< 6.2791e+07 23 5.059367e+16 1137483000  \n            88) n_hens< 6.0058e+07 9 1.218484e+16 1111544000 *\n            89) n_hens>=6.0058e+07 14 2.846117e+16 1154157000 *\n          45) n_hens>=6.2791e+07 15 1.292464e+16 1220853000 *\n        23) prod_process< 2 18 5.166236e+17 1369035000 *\n   3) n_hens>=1.82312e+08 38 4.578996e+18 7860687000  \n     6) n_hens< 3.23504e+08 20 1.426740e+18 7644080000  \n      12) n_hens< 3.13528e+08 7 1.529064e+17 7514314000 *\n      13) n_hens>=3.13528e+08 13 1.092489e+18 7713954000 *\n     7) n_hens>=3.23504e+08 18 1.171252e+18 8101361000 *\n\n\n\n#plot tree\nrpart.plot(extract_fit_parsnip(final_tree_fit)$fit)\n\nWarning: Cannot retrieve the data used to build the model (so cannot determine roundint and is.binary for the variables).\nTo silence this warning:\n    Call rpart.plot with roundint=FALSE,\n    or rebuild the rpart model with model=TRUE.\n\n\n\n\n\nLasso\nModel Specification\n\nlr_mod <- linear_reg(penalty = tune(), mixture = 1) %>%\n  set_engine(\"glmnet\")\n\nWorkflow Definition\n\nlr_workflow <- workflow() %>% \n  add_model(lr_mod) %>% \n  add_recipe(egg_rec)\n\nTuning Grid Specification\n\nlr_reg_grid <- tibble(penalty = 10^seq(-4, -1, length.out = 30))\nlr_reg_grid %>% top_n(-5)\n\nSelecting by penalty\n\n\n# A tibble: 5 × 1\n   penalty\n     <dbl>\n1 0.0001  \n2 0.000127\n3 0.000161\n4 0.000204\n5 0.000259\n\nlr_reg_grid %>% top_n(5)\n\nSelecting by penalty\n\n\n# A tibble: 5 × 1\n  penalty\n    <dbl>\n1  0.0386\n2  0.0489\n3  0.0621\n4  0.0788\n5  0.1   \n\n\nTuning using cross-validation and the tune_grid() function\n\nlr_res <- lr_workflow %>%\n  tune_grid(resamples = folds_train,\n            grid = lr_reg_grid,\n            control = control_grid(verbose = FALSE, save_pred = TRUE),\n            metrics = metric_set(rmse))\n\nlr_res %>% collect_metrics()\n\n# A tibble: 30 × 7\n    penalty .metric .estimator       mean     n   std_err .config              \n      <dbl> <chr>   <chr>           <dbl> <int>     <dbl> <chr>                \n 1 0.0001   rmse    standard   131030574.     4 18627216. Preprocessor1_Model01\n 2 0.000127 rmse    standard   131030574.     4 18627216. Preprocessor1_Model02\n 3 0.000161 rmse    standard   131030574.     4 18627216. Preprocessor1_Model03\n 4 0.000204 rmse    standard   131030574.     4 18627216. Preprocessor1_Model04\n 5 0.000259 rmse    standard   131030574.     4 18627216. Preprocessor1_Model05\n 6 0.000329 rmse    standard   131030574.     4 18627216. Preprocessor1_Model06\n 7 0.000418 rmse    standard   131030574.     4 18627216. Preprocessor1_Model07\n 8 0.000530 rmse    standard   131030574.     4 18627216. Preprocessor1_Model08\n 9 0.000672 rmse    standard   131030574.     4 18627216. Preprocessor1_Model09\n10 0.000853 rmse    standard   131030574.     4 18627216. Preprocessor1_Model10\n# … with 20 more rows\n\n\n\n#plot using autoplot\nlr_res %>% autoplot()\n\n\n\n\n\n#getting best model\nlr_res %>%\n  show_best(metric = \"rmse\")\n\n# A tibble: 5 × 7\n   penalty .metric .estimator       mean     n   std_err .config              \n     <dbl> <chr>   <chr>           <dbl> <int>     <dbl> <chr>                \n1 0.0001   rmse    standard   131030574.     4 18627216. Preprocessor1_Model01\n2 0.000127 rmse    standard   131030574.     4 18627216. Preprocessor1_Model02\n3 0.000161 rmse    standard   131030574.     4 18627216. Preprocessor1_Model03\n4 0.000204 rmse    standard   131030574.     4 18627216. Preprocessor1_Model04\n5 0.000259 rmse    standard   131030574.     4 18627216. Preprocessor1_Model05\n\nbest_lr <- lr_res %>%\n  select_best(metric = \"rmse\")\nbest_lr\n\n# A tibble: 1 × 2\n  penalty .config              \n    <dbl> <chr>                \n1  0.0001 Preprocessor1_Model01\n\n\n\n#finalizing workflows with best models\nfinal_lr_wf <- lr_workflow %>%\n  finalize_workflow(best_lr)\n\nfinal_lr_fit <- final_lr_wf %>% fit(data=egg_train)\nfinal_lr_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_dummy()\n• step_ordinalscore()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:  glmnet::glmnet(x = maybe_matrix(x), y = y, family = \"gaussian\",      alpha = ~1) \n\n   Df  %Dev    Lambda\n1   0  0.00 3.054e+09\n2   1 16.92 2.783e+09\n3   1 30.96 2.535e+09\n4   1 42.62 2.310e+09\n5   1 52.30 2.105e+09\n6   1 60.34 1.918e+09\n7   1 67.01 1.748e+09\n8   1 72.55 1.592e+09\n9   1 77.15 1.451e+09\n10  1 80.97 1.322e+09\n11  1 84.14 1.205e+09\n12  1 86.77 1.098e+09\n13  1 88.96 1.000e+09\n14  1 90.77 9.112e+08\n15  1 92.28 8.302e+08\n16  1 93.53 7.565e+08\n17  1 94.57 6.893e+08\n18  1 95.43 6.280e+08\n19  1 96.14 5.723e+08\n20  1 96.74 5.214e+08\n21  1 97.23 4.751e+08\n22  1 97.64 4.329e+08\n23  1 97.98 3.944e+08\n24  1 98.26 3.594e+08\n25  1 98.50 3.275e+08\n26  1 98.69 2.984e+08\n27  1 98.85 2.719e+08\n28  1 98.99 2.477e+08\n29  1 99.10 2.257e+08\n30  1 99.19 2.057e+08\n31  1 99.27 1.874e+08\n32  1 99.33 1.707e+08\n33  2 99.40 1.556e+08\n34  2 99.47 1.418e+08\n35  2 99.53 1.292e+08\n36  2 99.58 1.177e+08\n37  2 99.62 1.072e+08\n38  2 99.66 9.770e+07\n39  2 99.69 8.902e+07\n40  2 99.71 8.112e+07\n41  2 99.73 7.391e+07\n42  2 99.74 6.734e+07\n43  2 99.76 6.136e+07\n44  2 99.77 5.591e+07\n45  2 99.78 5.094e+07\n46  2 99.79 4.642e+07\n\n...\nand 11 more lines.\n\n\n\nx <- final_lr_fit$fit$fit$fit\nplot(x, \"lambda\")\n\n\n\n\nRandom Forest\nModel Specification\n\ncores <- parallel::detectCores()\ncores\n\n[1] 8\n\nrf_mod <-\n  rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>%\n  set_engine(\"ranger\", num.threads = cores) %>%\n  set_mode(\"regression\")\n\nWorkflow Definition\n\nrf_workflow <-\n  workflow() %>%\n  add_model(rf_mod) %>%\n  add_recipe(egg_rec)\n\nTuning Grid Specification\n\nrf_mod\n\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  mtry = tune()\n  trees = 1000\n  min_n = tune()\n\nEngine-Specific Arguments:\n  num.threads = cores\n\nComputational engine: ranger \n\nextract_parameter_set_dials(rf_mod)\n\nCollection of 2 parameters for tuning\n\n identifier  type    object\n       mtry  mtry nparam[?]\n      min_n min_n nparam[+]\n\nModel parameters needing finalization:\n   # Randomly Selected Predictors ('mtry')\n\nSee `?dials::finalize` or `?dials::update.parameters` for more information.\n\n\nTuning Using Cross-Validation and the Tune_Grid()\n\nrf_res <- rf_workflow %>%\n  tune_grid(resamples = folds_train,\n            grid = 25,\n            control = control_grid(save_pred = TRUE),\n            metrics = metric_set(rmse))\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\nrf_res %>%\n  collect_metrics()\n\n# A tibble: 25 × 8\n    mtry min_n .metric .estimator       mean     n   std_err .config            \n   <int> <int> <chr>   <chr>           <dbl> <int>     <dbl> <chr>              \n 1     1    15 rmse    standard   563073807.     4  9285171. Preprocessor1_Mode…\n 2     1    38 rmse    standard   835833579.     4 30546016. Preprocessor1_Mode…\n 3     2     9 rmse    standard   159344366.     4 14178110. Preprocessor1_Mode…\n 4     2    30 rmse    standard   204989854.     4 17781499. Preprocessor1_Mode…\n 5     2    35 rmse    standard   323388067.     4 37452917. Preprocessor1_Mode…\n 6     2    24 rmse    standard   193050821.     4 16605822. Preprocessor1_Mode…\n 7     1    22 rmse    standard   571019135.     4  7985768. Preprocessor1_Mode…\n 8     3    13 rmse    standard   155387375.     4 11396491. Preprocessor1_Mode…\n 9     1    40 rmse    standard   916733633.     4 33600372. Preprocessor1_Mode…\n10     3    28 rmse    standard   192461542.     4 17967922. Preprocessor1_Mode…\n# … with 15 more rows\n\n\n\n#plot with autoplot\nautoplot(rf_res)\n\n\n\n\n\n#getting best model\nrf_res %>%\n  show_best(metric = \"rmse\")\n\n# A tibble: 5 × 8\n   mtry min_n .metric .estimator       mean     n   std_err .config             \n  <int> <int> <chr>   <chr>           <dbl> <int>     <dbl> <chr>               \n1     3    13 rmse    standard   155387375.     4 11396491. Preprocessor1_Model…\n2     2     6 rmse    standard   159132226.     4 13589541. Preprocessor1_Model…\n3     2     9 rmse    standard   159344366.     4 14178110. Preprocessor1_Model…\n4     2    10 rmse    standard   160156976.     4 14588265. Preprocessor1_Model…\n5     2     3 rmse    standard   160637120.     4 12972958. Preprocessor1_Model…\n\nbest_rf <- rf_res %>%\n  select_best(metric = \"rmse\")\nbest_rf\n\n# A tibble: 1 × 3\n   mtry min_n .config              \n  <int> <int> <chr>                \n1     3    13 Preprocessor1_Model08\n\n\n\n#finalizing workflow with best model\nfinal_rf_wf <- rf_workflow %>%\n  finalize_workflow(best_rf)\n\nfinal_rf_fit <- final_rf_wf %>% fit(data=egg_train)\nfinal_rf_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_dummy()\n• step_ordinalscore()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, mtry = min_cols(~3L,      x), num.trees = ~1000, min.node.size = min_rows(~13L, x),      num.threads = ~cores, verbose = FALSE, seed = sample.int(10^5,          1)) \n\nType:                             Regression \nNumber of trees:                  1000 \nSample size:                      152 \nNumber of independent variables:  3 \nMtry:                             3 \nTarget node size:                 13 \nVariable importance mode:         none \nSplitrule:                        variance \nOOB prediction error (MSE):       2.31789e+16 \nR squared (OOB):                  0.9975399 \n\n\nPoisson\n\np_mod <- poisson_reg(penalty = tune(),\n                     mixture = 1, mode = \"regression\") %>%\n  set_engine(\"glmnet\")\np_workflow <- workflow() %>%\n  add_model(lr_mod) %>%\n  add_recipe(egg_rec)\n\n\np_reg_grid <- tibble(penalty = 10^seq(-4, -1, length.out = 30))\np_reg_grid %>% top_n(-5)\n\nSelecting by penalty\n\n\n# A tibble: 5 × 1\n   penalty\n     <dbl>\n1 0.0001  \n2 0.000127\n3 0.000161\n4 0.000204\n5 0.000259\n\np_reg_grid %>% top_n(5)\n\nSelecting by penalty\n\n\n# A tibble: 5 × 1\n  penalty\n    <dbl>\n1  0.0386\n2  0.0489\n3  0.0621\n4  0.0788\n5  0.1   \n\n\n\np_res <- p_workflow %>%\n  tune_grid(resamples = folds_train,\n            grid = p_reg_grid,\n            control = control_grid(verbose = FALSE, save_pred = TRUE),\n            metrics = metric_set(rmse))\np_res %>%\n  collect_metrics()\n\n# A tibble: 30 × 7\n    penalty .metric .estimator       mean     n   std_err .config              \n      <dbl> <chr>   <chr>           <dbl> <int>     <dbl> <chr>                \n 1 0.0001   rmse    standard   131030574.     4 18627216. Preprocessor1_Model01\n 2 0.000127 rmse    standard   131030574.     4 18627216. Preprocessor1_Model02\n 3 0.000161 rmse    standard   131030574.     4 18627216. Preprocessor1_Model03\n 4 0.000204 rmse    standard   131030574.     4 18627216. Preprocessor1_Model04\n 5 0.000259 rmse    standard   131030574.     4 18627216. Preprocessor1_Model05\n 6 0.000329 rmse    standard   131030574.     4 18627216. Preprocessor1_Model06\n 7 0.000418 rmse    standard   131030574.     4 18627216. Preprocessor1_Model07\n 8 0.000530 rmse    standard   131030574.     4 18627216. Preprocessor1_Model08\n 9 0.000672 rmse    standard   131030574.     4 18627216. Preprocessor1_Model09\n10 0.000853 rmse    standard   131030574.     4 18627216. Preprocessor1_Model10\n# … with 20 more rows\n\n\n\np_res %>%\n  autoplot()\n\n\n\n\n\np_res %>%\n  show_best(metric = \"rmse\")\n\n# A tibble: 5 × 7\n   penalty .metric .estimator       mean     n   std_err .config              \n     <dbl> <chr>   <chr>           <dbl> <int>     <dbl> <chr>                \n1 0.0001   rmse    standard   131030574.     4 18627216. Preprocessor1_Model01\n2 0.000127 rmse    standard   131030574.     4 18627216. Preprocessor1_Model02\n3 0.000161 rmse    standard   131030574.     4 18627216. Preprocessor1_Model03\n4 0.000204 rmse    standard   131030574.     4 18627216. Preprocessor1_Model04\n5 0.000259 rmse    standard   131030574.     4 18627216. Preprocessor1_Model05\n\nbest_p <- p_res %>%\n  select_best(metric = \"rmse\")\nbest_p\n\n# A tibble: 1 × 2\n  penalty .config              \n    <dbl> <chr>                \n1  0.0001 Preprocessor1_Model01\n\n\n\nfinal_p_workflow <- p_workflow %>%\n  finalize_workflow(best_p)\nfinal_p_fit <- final_p_workflow %>%\n  fit(data = egg_train)\nfinal_p_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_dummy()\n• step_ordinalscore()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:  glmnet::glmnet(x = maybe_matrix(x), y = y, family = \"gaussian\",      alpha = ~1) \n\n   Df  %Dev    Lambda\n1   0  0.00 3.054e+09\n2   1 16.92 2.783e+09\n3   1 30.96 2.535e+09\n4   1 42.62 2.310e+09\n5   1 52.30 2.105e+09\n6   1 60.34 1.918e+09\n7   1 67.01 1.748e+09\n8   1 72.55 1.592e+09\n9   1 77.15 1.451e+09\n10  1 80.97 1.322e+09\n11  1 84.14 1.205e+09\n12  1 86.77 1.098e+09\n13  1 88.96 1.000e+09\n14  1 90.77 9.112e+08\n15  1 92.28 8.302e+08\n16  1 93.53 7.565e+08\n17  1 94.57 6.893e+08\n18  1 95.43 6.280e+08\n19  1 96.14 5.723e+08\n20  1 96.74 5.214e+08\n21  1 97.23 4.751e+08\n22  1 97.64 4.329e+08\n23  1 97.98 3.944e+08\n24  1 98.26 3.594e+08\n25  1 98.50 3.275e+08\n26  1 98.69 2.984e+08\n27  1 98.85 2.719e+08\n28  1 98.99 2.477e+08\n29  1 99.10 2.257e+08\n30  1 99.19 2.057e+08\n31  1 99.27 1.874e+08\n32  1 99.33 1.707e+08\n33  2 99.40 1.556e+08\n34  2 99.47 1.418e+08\n35  2 99.53 1.292e+08\n36  2 99.58 1.177e+08\n37  2 99.62 1.072e+08\n38  2 99.66 9.770e+07\n39  2 99.69 8.902e+07\n40  2 99.71 8.112e+07\n41  2 99.73 7.391e+07\n42  2 99.74 6.734e+07\n43  2 99.76 6.136e+07\n44  2 99.77 5.591e+07\n45  2 99.78 5.094e+07\n46  2 99.79 4.642e+07\n\n...\nand 11 more lines.\n\n\n\ny <- final_p_fit$fit$fit$fit\nplot(y, \"lambda\")\n\n\n\n\n\n#comparing best models to null model to determine which model performed the best\ntree_res %>%\n  show_best(metric = \"rmse\", n=1)\n\n# A tibble: 1 × 8\n  cost_complexity tree_depth .metric .estimator       mean     n std_err .config\n            <dbl>      <int> <chr>   <chr>           <dbl> <int>   <dbl> <chr>  \n1    0.0000000001          8 rmse    standard   196540838.     4  2.07e7 Prepro…\n\nlr_res %>%\n  show_best(metric = \"rmse\", n=1)\n\n# A tibble: 1 × 7\n  penalty .metric .estimator       mean     n   std_err .config              \n    <dbl> <chr>   <chr>           <dbl> <int>     <dbl> <chr>                \n1  0.0001 rmse    standard   131030574.     4 18627216. Preprocessor1_Model01\n\nrf_res %>%\n  show_best(metric = \"rmse\", n=1)\n\n# A tibble: 1 × 8\n   mtry min_n .metric .estimator       mean     n   std_err .config             \n  <int> <int> <chr>   <chr>           <dbl> <int>     <dbl> <chr>               \n1     3    13 rmse    standard   155387375.     4 11396491. Preprocessor1_Model…\n\np_res %>%\n  show_best(metric = \"rmse\", n=1)\n\n# A tibble: 1 × 7\n  penalty .metric .estimator       mean     n   std_err .config              \n    <dbl> <chr>   <chr>           <dbl> <int>     <dbl> <chr>                \n1  0.0001 rmse    standard   131030574.     4 18627216. Preprocessor1_Model01\n\nnull_train_fit %>% \n  collect_metrics(metric = \"rmse\")\n\n# A tibble: 2 × 6\n  .metric .estimator        mean     n  std_err .config             \n  <chr>   <chr>            <dbl> <int>    <dbl> <chr>               \n1 rmse    standard   3059437142.     4 3035803. Preprocessor1_Model1\n2 rsq     standard          NaN      0      NA  Preprocessor1_Model1\n\n\nLASSO has the smallest RMSE so I will use it as my final model\nFinal evaluation\n\n#fitting lasso model to testing data with last_fit()\nlr_last_fit <- final_lr_wf %>%\n  last_fit(split)\n\nlr_last_fit %>% collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator     .estimate .config             \n  <chr>   <chr>              <dbl> <chr>               \n1 rmse    standard   119506902.    Preprocessor1_Model1\n2 rsq     standard           0.999 Preprocessor1_Model1\n\n#including null test metric for comparison\nnull_test_fit %>% collect_metrics()\n\n# A tibble: 2 × 6\n  .metric .estimator        mean     n   std_err .config             \n  <chr>   <chr>            <dbl> <int>     <dbl> <chr>               \n1 rmse    standard   3122622031.     4 44952823. Preprocessor1_Model1\n2 rsq     standard          NaN      0       NA  Preprocessor1_Model1\n\n\nThe last fit of LASSO with the testing has an even smaller RMSE than the best fit above and it also performed betting than the null model with testing data.\nSummary\nAfter importing the data and creating a recipe, I made a null model for comparison to the other models. I used regressions models so I could compare the RMSE of each. In my observations from this and previous assignments, the LASSO model seems to always perform better than the others in most situations."
  },
  {
    "objectID": "visualization_exercise.html",
    "href": "visualization_exercise.html",
    "title": "Visualization Exercise",
    "section": "",
    "text": "This data comes from FiveThirtyEight’s article https://projects.fivethirtyeight.com/college-fight-song-lyrics/. It contains stats on the fight songs of 65 schools – all those in the Power Five conferences (the ACC, Big Ten, Big 12, Pac-12 and SEC), plus Notre Dame– including duration, beats per minute, and mentions of typical themes of a fight song. I am going to try to recreate the graph of fight song beats per minute versus duration. The original graph is below (with UGA selected); it is interactive, allowing users to select a university to see where they fall on the fight song bpm vs duration scale as well as how many “fight song cliches” it contains.\n\nHere I load all the packages I need to recreate this graph and load the data provided by FiveThirtyEight.\n\n#load packages\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.0     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.1     ✔ tibble    3.1.8\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nlibrary(ggplot2)\nlibrary(plotly)\n\n\nAttaching package: 'plotly'\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\nThe following object is masked from 'package:stats':\n\n    filter\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\n\n\n#load the data\nfight_songs <-read_csv(\"data/fight-songs.csv\")\n\nRows: 65 Columns: 23\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (19): school, conference, song_name, writers, year, student_writer, offi...\ndbl  (4): bpm, sec_duration, number_fights, trope_count\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThe original graph includes intersecting lines with the average bpm and duration, so I took the mean of each column to determine those exact values so I could include them in my recreation.\n\nfight_songs %>% pull(bpm) %>% mean()\n\n[1] 128.8\n\nfight_songs %>% pull(sec_duration) %>% mean()\n\n[1] 71.90769\n\n\nNow to the graphing. I decided to create this graph in ggplot first and then make it interactive using the ggplotly() function from plotly.\n\ngraph <- fight_songs %>% \n  #color code each point by school name and include number of tropes\n  ggplot(aes(x=sec_duration, y=bpm, color=school, text=trope_count)) +\n  #make a scatter plot\n  geom_point(size=3, alpha=0.5) + \n  #include the average lines using the means I calculated above.\n  geom_vline(xintercept = 71.90769) + \n  geom_hline(yintercept = 128.8) + \n  #omit the legend \n  theme(legend.position=\"none\") + \n  #shift scale\n  scale_x_continuous(limits= c(0,180), breaks = seq(0,180,20))+\n  scale_y_continuous(limits= c(50,200), breaks= c(0,60,80,100,120,140,160,180,200)) +\n  #match the quadrant labels \n  annotate(\"text\", x = 20, y = 65, label = \"Slow but short\") + \n  annotate(\"text\", x = 20, y = 190, label = \"Fast and short\") + \n  annotate(\"text\", x = 135, y = 190, label = \"Fast but long\") + \n  annotate(\"text\", x = 135, y = 65, label = \"Slow and long\") + \n  #match the axes labels and title\n  labs(x=\"Duration\", y=\"Beats per Minute\") +\n  ggtitle(\"How Fight Songs Stack Up\")\n\n\nggplotly(graph)\n\n\n\n\n\nThere were two main things I struggled with in this recreation. I searched and searched for some “university color palette for R” but came up empty. The only options I was left with were using default ggplot colors or making my own using hex codes for each school. I decided that I would use default colors to save myself some sanity (and each school has its own color, just not its school color). I also could not figure out how to include a separate chart of sorts for the number of tropes in each song; after some searching, I found that the aes(text= ) in ggplot() mapping could be used to display the trope number in the hover menu after converting using ggplotly(). You can see in the final that Georgia does indeed have 1 fight song cliche in my recreation (the number at the bottom of the hover menu).\nI played around with ggvis for a few hours on my first attempt and, while I found it interesting with cool outputs, I had a hard time figuring out how to add all the elements that I needed to recreate this graph. Upon reading, I learned that ggvis is a sort of work-in-progress, and everything in ggplot does not have a direct translation to ggvis. I think ggvis could be used to make some really cool visualizations, but I was unsuccessful in using it for this exercise.\nOverall, this recreation is not an exact replica of the original but I think it turned out pretty good and I really enjoyed getting to play around with the interactive features of plotly()."
  }
]